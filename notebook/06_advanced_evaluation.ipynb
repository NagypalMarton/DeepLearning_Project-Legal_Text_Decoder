{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import könyvtárak\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba8077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(models_dir: Path):\n",
    "    \"\"\"Find and return path to best model checkpoint.\"\"\"\n",
    "    best_models = list(models_dir.glob('best_*.pt'))\n",
    "    if not best_models:\n",
    "        raise FileNotFoundError(f\"No best_*.pt checkpoints found in {models_dir}\")\n",
    "    final_balanced = [m for m in best_models if 'final_balanced' in m.name.lower()]\n",
    "    return final_balanced[0] if final_balanced else best_models[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ecc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    \"\"\"Dataset for transformer inference.\"\"\"\n",
    "    def __init__(self, texts: Sequence[str], tokenizer, max_length: int = 384):\n",
    "        self.texts = list(texts)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"text\": text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702f6d0",
   "metadata": {},
   "source": [
    "## Robustness Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_text(text: str, noise_level: float = 0.1) -> str:\n",
    "    \"\"\"Add simple character-level noise (delete/duplicate/space).\"\"\"\n",
    "    if not text or noise_level <= 0:\n",
    "        return text\n",
    "\n",
    "    text_list = list(text)\n",
    "    n_chars_to_modify = max(1, int(len(text_list) * noise_level))\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    for _ in range(n_chars_to_modify):\n",
    "        if not text_list:\n",
    "            break\n",
    "        idx = int(rng.integers(0, len(text_list)))\n",
    "        action = rng.choice([\"delete\", \"duplicate\", \"space\"])\n",
    "        if action == \"delete\":\n",
    "            text_list.pop(idx)\n",
    "        elif action == \"duplicate\" and idx < len(text_list):\n",
    "            text_list.insert(idx, text_list[idx])\n",
    "        else:\n",
    "            text_list[idx] = \" \"\n",
    "\n",
    "    return \"\".join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10fa83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text: str, ratio: float = 0.5) -> str:\n",
    "    \"\"\"Keep the first `ratio` portion of the words.\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    words = text.split()\n",
    "    n_words = max(1, int(len(words) * ratio))\n",
    "    return \" \".join(words[:n_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, tokenizer, texts: Sequence[str], device: torch.device, \n",
    "                  batch_size: int = 8, id2label: Dict[int, str] = None, max_length: int = 384):\n",
    "    \"\"\"Run batched inference.\"\"\"\n",
    "    dataset = TransformerDataset(texts, tokenizer, max_length=max_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=device.type == \"cuda\",\n",
    "    )\n",
    "\n",
    "    predictions: List[str] = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            if id2label:\n",
    "                predictions.extend([id2label[int(p)] for p in preds.cpu().numpy()])\n",
    "            else:\n",
    "                predictions.extend(preds.cpu().numpy().tolist())\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_robustness_results(results: Sequence[Dict], save_path: Path):\n",
    "    \"\"\"Plot bar chart of accuracies for perturbations.\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "\n",
    "    test_names = [r[\"test_name\"] for r in results]\n",
    "    accuracies = [r[\"accuracy\"] for r in results]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars = ax.bar(test_names, accuracies, color=\"steelblue\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(\"Model Robustness Tests\")\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.axhline(y=accuracies[0], color=\"red\", linestyle=\"--\", label=\"Baseline (Original)\")\n",
    "\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2.0, h, f\"{h:.3f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    print(f\"Robustness plot saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3319c4ab",
   "metadata": {},
   "source": [
    "## Explainability Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_based_importance(model, tokenizer, texts: Sequence[str], labels: Sequence[str],\n",
    "                                   device: torch.device, n_examples: int, max_length: int):\n",
    "    \"\"\"Extract top attended tokens for examples.\"\"\"\n",
    "    results = []\n",
    "    model.eval()\n",
    "    \n",
    "    for i, text in enumerate(texts[:n_examples]):\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask_tensor = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Note: This requires model to support output_attentions\n",
    "            # For custom models, you may need to modify the forward pass\n",
    "            try:\n",
    "                outputs = model.transformer(\n",
    "                    input_ids=input_ids, \n",
    "                    attention_mask=attention_mask_tensor, \n",
    "                    output_attentions=True, \n",
    "                    return_dict=True\n",
    "                )\n",
    "                attentions = outputs.attentions\n",
    "                \n",
    "                # Get model prediction\n",
    "                model_output = model(input_ids=input_ids, attention_mask=attention_mask_tensor)\n",
    "                logits = model_output.logits\n",
    "                pred_class = int(torch.argmax(logits, dim=1)[0])\n",
    "                probs = torch.softmax(logits, dim=1)[0]\n",
    "\n",
    "                avg_attention = torch.stack([att.mean(dim=1) for att in attentions]).mean(dim=0)[0]\n",
    "                tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "                attention_scores = avg_attention.mean(dim=0).cpu().numpy()\n",
    "\n",
    "                valid_tokens: List[str] = []\n",
    "                valid_scores: List[float] = []\n",
    "                for token, score in zip(tokens, attention_scores):\n",
    "                    if token not in [\"[PAD]\", \"[CLS]\", \"[SEP]\"]:\n",
    "                        valid_tokens.append(token)\n",
    "                        valid_scores.append(float(score))\n",
    "\n",
    "                top_indices = np.argsort(valid_scores)[-10:][::-1]\n",
    "                top_tokens = [\n",
    "                    (valid_tokens[idx], valid_scores[idx]) for idx in top_indices if idx < len(valid_tokens)\n",
    "                ]\n",
    "\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"example_id\": i,\n",
    "                        \"text_preview\": text[:200] + (\"...\" if len(text) > 200 else \"\"),\n",
    "                        \"true_label\": str(labels[i]),\n",
    "                        \"predicted_class_id\": pred_class,\n",
    "                        \"prediction_probabilities\": probs.cpu().numpy().tolist(),\n",
    "                        \"top_attended_tokens\": top_tokens,\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Attention extraction failed for example {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2499ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_misclassifications(y_true: Sequence[str], y_pred: Sequence[str], texts: Sequence[str]):\n",
    "    \"\"\"Analyze misclassified examples.\"\"\"\n",
    "    misclassified = []\n",
    "    for idx, (pred, true) in enumerate(zip(y_pred, y_true)):\n",
    "        if pred != true:\n",
    "            misclassified.append(\n",
    "                {\n",
    "                    \"index\": idx,\n",
    "                    \"text\": texts[idx][:200] + (\"...\" if len(texts[idx]) > 200 else \"\"),\n",
    "                    \"true_label\": str(true),\n",
    "                    \"predicted_label\": str(pred),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    confusion_pairs: Dict = {}\n",
    "    for item in misclassified:\n",
    "        pair = (item[\"true_label\"], item[\"predicted_label\"])\n",
    "        confusion_pairs[pair] = confusion_pairs.get(pair, 0) + 1\n",
    "\n",
    "    sorted_pairs = sorted(confusion_pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "    return {\n",
    "        \"total_misclassified\": len(misclassified),\n",
    "        \"total_examples\": len(texts),\n",
    "        \"error_rate\": (len(misclassified) / len(texts)) if texts else 0.0,\n",
    "        \"confusion_pairs\": [\n",
    "            {\"true_label\": p[0], \"predicted_label\": p[1], \"count\": c}\n",
    "            for p, c in sorted_pairs[:10]\n",
    "        ],\n",
    "        \"examples\": misclassified[:10],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c8bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_pairs(confusion_pairs: Sequence[Dict], save_path: Path, top_n: int = 10):\n",
    "    \"\"\"Plot top confusion pairs.\"\"\"\n",
    "    if not confusion_pairs:\n",
    "        return\n",
    "\n",
    "    pairs = confusion_pairs[:top_n]\n",
    "    pair_labels = [f\"{p['true_label']} → {p['predicted_label']}\" for p in pairs]\n",
    "    counts = [p[\"count\"] for p in pairs]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.barh(pair_labels, counts, color=\"coral\")\n",
    "    ax.set_xlabel(\"Count\")\n",
    "    ax.set_title(\"Top Confusion Pairs - Transformer\")\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    for i, count in enumerate(counts):\n",
    "        ax.text(count, i, f\" {count}\", va=\"center\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    print(f\"Confusion pairs plot saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e59b0dc",
   "metadata": {},
   "source": [
    "## Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fc289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "output_root = Path(os.getenv('OUTPUT_DIR', '../output'))\n",
    "models_dir = output_root / \"models\"\n",
    "data_csv = str(output_root / \"processed\" / \"test.csv\")\n",
    "\n",
    "batch_size = 8\n",
    "max_length = 384\n",
    "sample_count = 10  # For attention analysis\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(f\"Loading data from {data_csv}...\")\n",
    "df = pd.read_csv(data_csv)\n",
    "texts = df[\"text\"].astype(str).tolist()\n",
    "labels = df[\"label\"].astype(str).tolist()\n",
    "\n",
    "print(f\"Loaded {len(texts)} test samples\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203fff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (simplified - you'd need the actual model class)\n",
    "# Import BalancedFinalModel from 04 or define it here\n",
    "import torch.nn as nn\n",
    "\n",
    "class BalancedFinalModel(nn.Module):\n",
    "    \"\"\"Final: Balanced model.\"\"\"\n",
    "    def __init__(self, transformer_model, num_classes=5, hidden_dim=256, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.num_classes = num_classes\n",
    "        trans_hidden = transformer_model.config.hidden_size\n",
    "        \n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(trans_hidden, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.classifier = nn.Linear(hidden_dim // 2, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "        hidden = outputs.last_hidden_state\n",
    "        \n",
    "        mask = attention_mask.unsqueeze(-1)\n",
    "        summed = (hidden * mask).sum(1)\n",
    "        counts = mask.sum(1).clamp(min=1)\n",
    "        pooled = summed / counts\n",
    "        \n",
    "        adapted = self.adapter(pooled)\n",
    "        logits = self.classifier(adapted)\n",
    "        \n",
    "        output = type('Output', (), {'logits': logits})()\n",
    "        if labels is not None:\n",
    "            output.loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return output\n",
    "\n",
    "checkpoint_path = find_best_model(models_dir)\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "id2label = {int(k): v for k, v in checkpoint['id2label'].items()}\n",
    "\n",
    "transformer_model_name = os.getenv('TRANSFORMER_MODEL', 'SZTAKI-HLT/hubert-base-cc')\n",
    "base_transformer = AutoModel.from_pretrained(transformer_model_name)\n",
    "model = BalancedFinalModel(base_transformer, num_classes=len(id2label))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4458ac7",
   "metadata": {},
   "source": [
    "## Robustness Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8afcabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define perturbations\n",
    "perturbations = [\n",
    "    {\"name\": \"Original\", \"func\": lambda x, **_: x, \"params\": {}},\n",
    "    {\"name\": \"Noise 5%\", \"func\": add_noise_to_text, \"params\": {\"noise_level\": 0.05}},\n",
    "    {\"name\": \"Noise 10%\", \"func\": add_noise_to_text, \"params\": {\"noise_level\": 0.10}},\n",
    "    {\"name\": \"Noise 20%\", \"func\": add_noise_to_text, \"params\": {\"noise_level\": 0.20}},\n",
    "    {\"name\": \"Truncate 75%\", \"func\": truncate_text, \"params\": {\"ratio\": 0.75}},\n",
    "    {\"name\": \"Truncate 50%\", \"func\": truncate_text, \"params\": {\"ratio\": 0.50}},\n",
    "    {\"name\": \"Truncate 25%\", \"func\": truncate_text, \"params\": {\"ratio\": 0.25}},\n",
    "]\n",
    "\n",
    "robustness_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b379438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run robustness tests\n",
    "for test in perturbations:\n",
    "    name = test[\"name\"]\n",
    "    func: Callable = test[\"func\"]\n",
    "    params = test.get(\"params\", {})\n",
    "    print(f\"Running robustness test: {name} ({params})\")\n",
    "    \n",
    "    transformed = [func(t, **params) for t in texts]\n",
    "    y_pred = predict_batch(model, tokenizer, transformed, device, batch_size=batch_size, \n",
    "                          id2label=id2label, max_length=max_length)\n",
    "\n",
    "    labels_unique = sorted(list(set(labels) | set(y_pred)))\n",
    "    report = classification_report(labels, y_pred, labels=labels_unique, output_dict=True, zero_division=0)\n",
    "    \n",
    "    label2id_local = {label: idx for idx, label in enumerate(labels_unique)}\n",
    "    y_true_idx = [label2id_local[l] for l in labels]\n",
    "    y_pred_idx = [label2id_local[l] for l in y_pred]\n",
    "    macro_f1 = f1_score(y_true_idx, y_pred_idx, average=\"macro\", zero_division=0)\n",
    "    weighted_f1 = f1_score(y_true_idx, y_pred_idx, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    robustness_results.append(\n",
    "        {\n",
    "            \"test_name\": name,\n",
    "            \"accuracy\": float(accuracy_score(labels, y_pred)),\n",
    "            \"macro_f1\": float(macro_f1),\n",
    "            \"weighted_f1\": float(weighted_f1),\n",
    "            \"classification_report\": report,\n",
    "            \"transformation\": params,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"  Accuracy: {robustness_results[-1]['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d29019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot robustness results\n",
    "robustness_dir = output_root / \"advanced\" / \"robustness\"\n",
    "robustness_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "json_path = robustness_dir / \"robustness_results.json\"\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(robustness_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Robustness results saved to {json_path}\")\n",
    "\n",
    "plot_robustness_results(robustness_results, robustness_dir / \"robustness_accuracy.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da368025",
   "metadata": {},
   "source": [
    "## Explainability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69794b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions for explainability\n",
    "print(\"Running predictions for explainability...\")\n",
    "y_pred = predict_batch(model, tokenizer, texts, device, batch_size=batch_size, \n",
    "                      id2label=id2label, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdacbd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention-based importance\n",
    "print(\"Extracting attention-based importance...\")\n",
    "try:\n",
    "    attention_results = get_attention_based_importance(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        texts=texts,\n",
    "        labels=labels,\n",
    "        device=device,\n",
    "        n_examples=min(sample_count, len(texts)),\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    print(f\"Extracted attention for {len(attention_results)} examples\")\n",
    "except Exception as exc:\n",
    "    print(f\"Attention extraction failed: {exc}\")\n",
    "    attention_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0758372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save attention results\n",
    "explain_dir = output_root / \"advanced\" / \"explainability\"\n",
    "explain_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if attention_results:\n",
    "    attention_path = explain_dir / \"attention_importance.json\"\n",
    "    with open(attention_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(attention_results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Attention importance saved to {attention_path}\")\n",
    "    \n",
    "    # Display first example\n",
    "    print(\"\\nExample attention analysis:\")\n",
    "    display(pd.DataFrame([attention_results[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa36e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassifications\n",
    "print(\"Analyzing misclassifications...\")\n",
    "misclass = analyze_misclassifications(labels, y_pred, texts)\n",
    "\n",
    "print(f\"\\nMisclassification Summary:\")\n",
    "print(f\"Total misclassified: {misclass['total_misclassified']} / {misclass['total_examples']}\")\n",
    "print(f\"Error rate: {misclass['error_rate']:.2%}\")\n",
    "\n",
    "misclass_path = explain_dir / \"misclassification_analysis.json\"\n",
    "with open(misclass_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(misclass, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Misclassification analysis saved to {misclass_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion pairs\n",
    "if misclass.get(\"confusion_pairs\"):\n",
    "    plot_confusion_pairs(misclass[\"confusion_pairs\"], explain_dir / \"confusion_pairs.png\")\n",
    "    \n",
    "    # Display top confusion pairs\n",
    "    print(\"\\nTop Confusion Pairs:\")\n",
    "    display(pd.DataFrame(misclass[\"confusion_pairs\"][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed07690a",
   "metadata": {},
   "source": [
    "## Advanced Evaluation Complete!\n",
    "\n",
    "Robustness testing és explainability analysis befejezve."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
