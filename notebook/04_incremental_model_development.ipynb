{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35881eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import könyvtárak\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import amp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89046f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53203724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalTextDataset(Dataset):\n",
    "    \"\"\"Dataset for legal text classification.\"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba30de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(raw):\n",
    "    s = str(raw).strip()\n",
    "    m = re.match(r'^([1-5])', s)\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "\n",
    "def build_ordinal_mapping(labels):\n",
    "    numeric = [normalize_label(l) for l in labels]\n",
    "    unique = sorted(set(numeric))\n",
    "    label2id = {u: i for i, u in enumerate(unique)}\n",
    "    id2label = {label2id[u]: str(u) for u in unique}\n",
    "    encoded = [label2id[n] for n in numeric]\n",
    "    return encoded, label2id, id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911fb81b",
   "metadata": {},
   "source": [
    "## Progressive Model Architectures\n",
    "\n",
    "4 különböző modell architektúra definiálása:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step1_BaselineModel(nn.Module):\n",
    "    \"\"\"Step 1: Minimal baseline - Transformer + single linear classifier.\"\"\"\n",
    "    def __init__(self, transformer_model, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.num_classes = num_classes\n",
    "        hidden_size = transformer_model.config.hidden_size\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "        pooled = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        output = type('Output', (), {'logits': logits})()\n",
    "        if labels is not None:\n",
    "            output.loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step2_ExtendedModel(nn.Module):\n",
    "    \"\"\"Step 2: Extended - 2-layer adapter + BatchNorm + Dropout.\"\"\"\n",
    "    def __init__(self, transformer_model, num_classes=5, hidden_dim=256, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.num_classes = num_classes\n",
    "        trans_hidden = transformer_model.config.hidden_size\n",
    "        \n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(trans_hidden, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.classifier = nn.Linear(hidden_dim // 2, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "        pooled = outputs.last_hidden_state[:, 0, :]\n",
    "        adapted = self.adapter(pooled)\n",
    "        logits = self.classifier(adapted)\n",
    "        \n",
    "        output = type('Output', (), {'logits': logits})()\n",
    "        if labels is not None:\n",
    "            output.loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adeb3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step3_AdvancedModel(nn.Module):\n",
    "    \"\"\"Step 3: Advanced - Attention pooling + 3-layer adapter + gating.\"\"\"\n",
    "    def __init__(self, transformer_model, num_classes=5, hidden_dim=256, dropout=0.4, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.num_classes = num_classes\n",
    "        trans_hidden = transformer_model.config.hidden_size\n",
    "        \n",
    "        # Attention pooling\n",
    "        self.attention_pool = nn.MultiheadAttention(\n",
    "            embed_dim=trans_hidden,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, trans_hidden))\n",
    "        \n",
    "        # 3-layer adapter\n",
    "        self.adapter_1 = nn.Sequential(\n",
    "            nn.Linear(trans_hidden, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.adapter_2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.adapter_3 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.gate = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim // 2, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "        hidden = outputs.last_hidden_state\n",
    "        \n",
    "        # Attention pooling\n",
    "        batch_size = hidden.size(0)\n",
    "        query = self.query.expand(batch_size, -1, -1)\n",
    "        pooled, _ = self.attention_pool(query, hidden, hidden, key_padding_mask=~attention_mask.bool())\n",
    "        pooled = pooled.squeeze(1)\n",
    "        \n",
    "        # 3-layer adapter with gating\n",
    "        x = self.adapter_1(pooled)\n",
    "        residual = x\n",
    "        x = self.adapter_2(x)\n",
    "        gate = torch.sigmoid(self.gate(x))\n",
    "        x = gate * x + (1 - gate) * residual\n",
    "        x = self.adapter_3(x)\n",
    "        \n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        output = type('Output', (), {'logits': logits})()\n",
    "        if labels is not None:\n",
    "            output.loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b44a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedFinalModel(nn.Module):\n",
    "    \"\"\"Final: Balanced - Production-ready model with best practices.\"\"\"\n",
    "    def __init__(self, transformer_model, num_classes=5, hidden_dim=256, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.num_classes = num_classes\n",
    "        trans_hidden = transformer_model.config.hidden_size\n",
    "        \n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(trans_hidden, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.classifier = nn.Linear(hidden_dim // 2, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "        hidden = outputs.last_hidden_state\n",
    "        \n",
    "        # Mean pooling\n",
    "        mask = attention_mask.unsqueeze(-1)\n",
    "        summed = (hidden * mask).sum(1)\n",
    "        counts = mask.sum(1).clamp(min=1)\n",
    "        pooled = summed / counts\n",
    "        \n",
    "        adapted = self.adapter(pooled)\n",
    "        logits = self.classifier(adapted)\n",
    "        \n",
    "        output = type('Output', (), {'logits': logits})()\n",
    "        if labels is not None:\n",
    "            output.loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb74ef",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ffb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, device, criterion=None, grad_acc_steps=1):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    disable_tqdm = not sys.stdout.isatty()\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", disable=disable_tqdm)\n",
    "    scaler = amp.GradScaler('cuda', enabled=device.type == 'cuda')\n",
    "    step_count = 0\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "        labels = batch['label'].to(device, non_blocking=True)\n",
    "        \n",
    "        with amp.autocast('cuda', enabled=scaler.is_enabled()):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            if criterion is None:\n",
    "                outputs_with_labels = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs_with_labels.loss / grad_acc_steps\n",
    "            else:\n",
    "                loss = criterion(logits, labels) / grad_acc_steps\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        step_count += 1\n",
    "        \n",
    "        if step_count % grad_acc_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item() * grad_acc_steps\n",
    "        preds = torch.argmax(logits.detach(), dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        progress_bar.set_postfix({'loss': (total_loss / step_count)})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, criterion=None):\n",
    "    \"\"\"Evaluate model on validation/test set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        disable_tqdm = not sys.stdout.isatty()\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluating\", disable=disable_tqdm)\n",
    "        mixed = device.type == 'cuda'\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "            labels = batch['label'].to(device, non_blocking=True)\n",
    "            \n",
    "            with amp.autocast('cuda', enabled=mixed):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                if criterion is None:\n",
    "                    outputs_with_labels = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs_with_labels.loss\n",
    "                else:\n",
    "                    loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits.detach(), dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    return avg_loss, accuracy, predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1689cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss')\n",
    "    if 'val_loss' in history:\n",
    "        ax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(epochs, history['train_acc'], 'b-', label='Train Accuracy')\n",
    "    if 'val_acc' in history:\n",
    "        ax2.plot(epochs, history['val_acc'], 'r-', label='Val Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3f64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(all_results, save_path):\n",
    "    \"\"\"Plot comparison of all 4 models' performance.\"\"\"\n",
    "    model_names = list(all_results.keys())\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Val Accuracy Comparison\n",
    "    val_accs = [all_results[name]['val_acc'] for name in model_names]\n",
    "    train_accs = [all_results[name]['train_acc'] for name in model_names]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    axes[0, 0].bar(x - width/2, train_accs, width, label='Train Acc', color='#3498db')\n",
    "    axes[0, 0].bar(x + width/2, val_accs, width, label='Val Acc', color='#e74c3c')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].set_title('Train vs Val Accuracy Comparison')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(model_names, rotation=15, ha='right')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overfitting Gap\n",
    "    gaps = [all_results[name]['train_acc'] - all_results[name]['val_acc'] for name in model_names]\n",
    "    colors = ['#2ecc71' if g < 0.05 else '#f39c12' if g < 0.10 else '#e74c3c' for g in gaps]\n",
    "    axes[0, 1].bar(model_names, gaps, color=colors)\n",
    "    axes[0, 1].axhline(y=0.05, color='orange', linestyle='--', label='5% threshold')\n",
    "    axes[0, 1].axhline(y=0.10, color='red', linestyle='--', label='10% threshold')\n",
    "    axes[0, 1].set_ylabel('Train - Val Accuracy Gap')\n",
    "    axes[0, 1].set_title('Overfitting Analysis')\n",
    "    axes[0, 1].set_xticks(range(len(model_names)))\n",
    "    axes[0, 1].set_xticklabels(model_names, rotation=15, ha='right')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Convergence Speed\n",
    "    epochs_to_best = [all_results[name]['epochs_trained'] for name in model_names]\n",
    "    axes[1, 0].bar(model_names, epochs_to_best, color='#9b59b6')\n",
    "    axes[1, 0].set_ylabel('Epochs')\n",
    "    axes[1, 0].set_title('Training Epochs')\n",
    "    axes[1, 0].set_xticks(range(len(model_names)))\n",
    "    axes[1, 0].set_xticklabels(model_names, rotation=15, ha='right')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 Scores\n",
    "    macro_f1s = [all_results[name]['val_macro_f1'] for name in model_names]\n",
    "    weighted_f1s = [all_results[name]['val_weighted_f1'] for name in model_names]\n",
    "    \n",
    "    axes[1, 1].bar(x - width/2, macro_f1s, width, label='Macro F1', color='#1abc9c')\n",
    "    axes[1, 1].bar(x + width/2, weighted_f1s, width, label='Weighted F1', color='#34495e')\n",
    "    axes[1, 1].set_ylabel('F1 Score')\n",
    "    axes[1, 1].set_title('F1 Score Comparison')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(model_names, rotation=15, ha='right')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    print(f\"Model comparison plot saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ef4a12",
   "metadata": {},
   "source": [
    "## Main Training Pipeline\n",
    "\n",
    "Betöltjük az adatokat és tanítjuk mind a 4 modellt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd55589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "base_output = os.getenv('OUTPUT_DIR', '../output')\n",
    "processed_dir = os.path.join(base_output, 'processed')\n",
    "models_dir = os.path.join(base_output, 'models')\n",
    "reports_dir = os.path.join(base_output, 'reports')\n",
    "\n",
    "Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(reports_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "model_name = os.getenv('TRANSFORMER_MODEL', 'SZTAKI-HLT/hubert-base-cc')\n",
    "batch_size = 8\n",
    "epochs = 15\n",
    "learning_rate = 1.5e-5\n",
    "weight_decay = 0.01\n",
    "max_length = 320\n",
    "label_smoothing = 0.02\n",
    "early_stopping_patience = 3\n",
    "grad_acc_steps = 2\n",
    "\n",
    "print(f\"Base Model: {model_name}\")\n",
    "print(f\"Batch Size: {batch_size} | Epochs: {epochs} | LR: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b8f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_path = os.path.join(processed_dir, \"train.csv\")\n",
    "val_path = os.path.join(processed_dir, \"val.csv\")\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(val_path) if os.path.exists(val_path) else None\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "if val_df is not None:\n",
    "    print(f\"Val samples: {len(val_df)}\")\n",
    "\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dcf28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels\n",
    "y_train_str = train_df['label'].astype(str).tolist()\n",
    "y_train, label2id, id2label = build_ordinal_mapping(y_train_str)\n",
    "\n",
    "# Save label mapping\n",
    "label_map_path = os.path.join(models_dir, 'label_mapping.json')\n",
    "with open(label_map_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump({'label2id': label2id, 'id2label': id2label}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "num_labels = len(label2id)\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(f\"Label mapping: {label2id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de898601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"GPU: {gpu_name} | Total VRAM: {total_mem:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0890cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prepare datasets\n",
    "X_train = train_df['text'].astype(str).tolist()\n",
    "train_dataset = LegalTextDataset(X_train, y_train, tokenizer, max_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=device.type=='cuda',\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = None\n",
    "if val_df is not None:\n",
    "    y_val_str = val_df['label'].astype(str).tolist()\n",
    "    y_val_numeric = [normalize_label(label) for label in y_val_str]\n",
    "    y_val = [label2id[n] for n in y_val_numeric]\n",
    "    X_val = val_df['text'].astype(str).tolist()\n",
    "    val_dataset = LegalTextDataset(X_val, y_val, tokenizer, max_length)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=device.type=='cuda',\n",
    "        num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b0803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "class_weights_raw = compute_class_weight('balanced', classes=np.unique(y_train), y=np.array(y_train))\n",
    "class_weights = np.sqrt(class_weights_raw)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(f\"Class weights (sqrt-scaled): {class_weights}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=label_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d50461",
   "metadata": {},
   "source": [
    "## Train All 4 Models\n",
    "\n",
    "Most tanítjuk mind a 4 modell architektúrát és összehasonlítjuk őket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f18345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configs\n",
    "model_configs = [\n",
    "    {\n",
    "        'name': 'Step1_Baseline',\n",
    "        'class': Step1_BaselineModel,\n",
    "        'description': 'Minimal baseline: Transformer + single linear classifier'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Step2_Extended',\n",
    "        'class': Step2_ExtendedModel,\n",
    "        'description': '2-layer adapter + BatchNorm + Dropout(0.3)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Step3_Advanced',\n",
    "        'class': Step3_AdvancedModel,\n",
    "        'description': 'Attention pooling + 3-layer adapter + gating mechanism'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Final_Balanced',\n",
    "        'class': BalancedFinalModel,\n",
    "        'description': 'PRODUCTION RECOMMENDED: Mean pooling + balanced architecture'\n",
    "    }\n",
    "]\n",
    "\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each model (simplified training loop for notebook)\n",
    "for config in model_configs:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training {config['name']}\")\n",
    "    print(f\"Architecture: {config['description']}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Load fresh base transformer\n",
    "    base_transformer = AutoModel.from_pretrained(model_name)\n",
    "    model = config['class'](base_transformer, num_classes=num_labels)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    effective_steps_per_epoch = math.ceil(len(train_loader) / max(1, grad_acc_steps))\n",
    "    total_steps = effective_steps_per_epoch * epochs\n",
    "    warmup_steps = int(0.15 * total_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    \n",
    "    # Training loop\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_macro_f1': [], 'val_weighted_f1': []}\n",
    "    best_metric_val = -float('inf')\n",
    "    no_improve_epochs = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device, criterion, grad_acc_steps)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "        \n",
    "        if val_loader is not None:\n",
    "            val_loss, val_acc, val_preds, val_trues = evaluate(model, val_loader, device, criterion)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            val_macro_f1 = f1_score(val_trues, val_preds, average='macro')\n",
    "            val_weighted_f1 = f1_score(val_trues, val_preds, average='weighted')\n",
    "            history['val_macro_f1'].append(val_macro_f1)\n",
    "            history['val_weighted_f1'].append(val_weighted_f1)\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val Weighted-F1: {val_weighted_f1:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            current = val_weighted_f1\n",
    "            if current > best_metric_val + 1e-4:\n",
    "                best_metric_val = current\n",
    "                no_improve_epochs = 0\n",
    "                print(f\"✓ New best (val_weighted_f1 = {current:.4f})\")\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "                if no_improve_epochs >= early_stopping_patience:\n",
    "                    print(f\"Early stopping triggered\")\n",
    "                    break\n",
    "    \n",
    "    # Plot training history\n",
    "    history_plot_path = os.path.join(reports_dir, f'04-{config[\"name\"].lower().replace(\" \", \"_\")}_history.png')\n",
    "    plot_training_history(history, history_plot_path)\n",
    "    \n",
    "    # Store results\n",
    "    all_results[config['name']] = {\n",
    "        'name': config['name'],\n",
    "        'train_acc': history['train_acc'][-1],\n",
    "        'val_acc': history['val_acc'][-1] if history['val_acc'] else 0.0,\n",
    "        'val_macro_f1': history['val_macro_f1'][-1] if history['val_macro_f1'] else 0.0,\n",
    "        'val_weighted_f1': best_metric_val,\n",
    "        'epochs_trained': len(history['train_acc'])\n",
    "    }\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del base_transformer, model\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b924404",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d811035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FINAL RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Train Acc: {result['train_acc']:.4f} | Val Acc: {result['val_acc']:.4f}\")\n",
    "    print(f\"  Val Weighted F1: {result['val_weighted_f1']:.4f} | Val Macro F1: {result['val_macro_f1']:.4f}\")\n",
    "    print(f\"  Epochs Trained: {result['epochs_trained']}\")\n",
    "    print(f\"  Gap (Train-Val): {result['train_acc'] - result['val_acc']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Save summary JSON\n",
    "summary_path = os.path.join(reports_dir, '04-expansion_summary.json')\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Summary saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba136ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison plot\n",
    "comparison_plot_path = os.path.join(reports_dir, '04-model_expansion_comparison.png')\n",
    "plot_model_comparison(all_results, comparison_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af344d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model\n",
    "best_name, best_result = max(all_results.items(), key=lambda kv: kv[1]['val_weighted_f1'])\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST MODEL: {best_name}\")\n",
    "print(f\"Val Weighted F1: {best_result['val_weighted_f1']:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
