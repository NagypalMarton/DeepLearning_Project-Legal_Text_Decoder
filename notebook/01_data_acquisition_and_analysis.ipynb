{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bfcbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import könyvtárak\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "from collections import Counter\n",
    "from urllib.request import urlretrieve, Request, urlopen\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(file_path: str):\n",
    "    \"\"\"Load data from a single JSON file and return its parsed content.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: JSON file not found at {file_path}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in {file_path}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ac597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_data(data_dir: str, download_url: str) -> bool:\n",
    "    \"\"\"\n",
    "    Download ZIP file from SharePoint and extract JSON files to data_dir.\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading data from SharePoint...\")\n",
    "        \n",
    "        # Create temporary directory for download\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            zip_path = os.path.join(temp_dir, 'data.zip')\n",
    "            \n",
    "            # Download file with custom headers for SharePoint\n",
    "            print(f\"Downloading... (this may take a moment)\")\n",
    "            \n",
    "            # Create request with headers\n",
    "            req = Request(download_url)\n",
    "            req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "            req.add_header('Referer', 'https://bmeedu-my.sharepoint.com/')\n",
    "            req.add_header('Accept', '*/*')\n",
    "            \n",
    "            # Download with progress\n",
    "            with urlopen(req, timeout=60) as response:\n",
    "                content_type = response.headers.get('Content-Type', '')\n",
    "                data = response.read()\n",
    "                \n",
    "                # Check if it's actually a ZIP file (starts with PK magic number)\n",
    "                if not data.startswith(b'PK'):\n",
    "                    print(f\"   ✗ Non-ZIP content (Content-Type: {content_type}, Size: {len(data)} bytes)\")\n",
    "                    return False\n",
    "                \n",
    "                with open(zip_path, 'wb') as out_file:\n",
    "                    out_file.write(data)\n",
    "            \n",
    "            print(f\"✓ Download complete: {os.path.getsize(zip_path)} bytes\")\n",
    "            \n",
    "            # Extract ZIP\n",
    "            print(f\"Extracting ZIP file...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(temp_dir)\n",
    "            print(f\"✓ Extraction complete\")\n",
    "            \n",
    "            # Find and copy all JSON files from subdirectories\n",
    "            json_count = 0\n",
    "            for root, dirs, files in os.walk(temp_dir):\n",
    "                for file in files:\n",
    "                    if file.endswith('.json'):\n",
    "                        src_path = os.path.join(root, file)\n",
    "                        dst_path = os.path.join(data_dir, file)\n",
    "                        shutil.copy2(src_path, dst_path)\n",
    "                        json_count += 1\n",
    "            \n",
    "            print(f\"✓ Copied {json_count} JSON files to {data_dir}\")\n",
    "            return json_count > 0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error downloading/extracting data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ccda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_data_available(data_dir: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if data_dir has JSON files. If empty, download from SharePoint.\n",
    "    Returns True if data is available, False otherwise.\n",
    "    \"\"\"\n",
    "    # Create data directory if it doesn't exist\n",
    "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if any JSON files exist\n",
    "    json_files = glob.glob(os.path.join(data_dir, '*.json'))\n",
    "    \n",
    "    if json_files:\n",
    "        print(f\"✓ Data directory contains {len(json_files)} JSON files\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"⚠ Data directory is empty or has no JSON files\")\n",
    "    print(f\"Attempting to download data from SharePoint...\")\n",
    "    \n",
    "    # Try different SharePoint URL approaches\n",
    "    urls = [\n",
    "        \"https://bmeedu-my.sharepoint.com/personal/gyires-toth_balint_vik_bme_hu/_layouts/15/download.aspx?share=IQDYwXUJcB_jQYr0bDfNT5RKARYgfKoH97zho3rxZ46KA1I\",\n",
    "        \"https://bmeedu-my.sharepoint.com/:u:/g/personal/gyires-toth_balint_vik_bme_hu/IQDYwXUJcB_jQYr0bDfNT5RKARYgfKoH97zho3rxZ46KA1I?e=iFp3iz&download=1\",\n",
    "    ]\n",
    "    \n",
    "    for url in urls:\n",
    "        success = download_and_extract_data(data_dir, url)\n",
    "        if success:\n",
    "            print(f\"✓ Data downloaded and extracted successfully\")\n",
    "            return True\n",
    "    \n",
    "    # All attempts failed\n",
    "    print(f\"✗ Failed to download data. Please provide data files in {data_dir}\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbfb1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_items(input_path: str) -> List[Union[dict, list]]:\n",
    "    \"\"\"Load items from a JSON file or from all JSON files in a directory.\n",
    "\n",
    "    - If input_path is a directory: loads all *.json files and concatenates lists.\n",
    "    - If input_path is a file: loads that JSON.\n",
    "    Returns a list of items (dicts) with source file annotated.\n",
    "    \"\"\"\n",
    "    if os.path.isdir(input_path):\n",
    "        print(f\"Loading all JSON files from directory: {input_path}\")\n",
    "        items: List[dict] = []\n",
    "        json_files = sorted(glob.glob(os.path.join(input_path, '*.json')))\n",
    "        if not json_files:\n",
    "            raise FileNotFoundError(f\"No JSON files found in directory: {input_path}\")\n",
    "        for fp in json_files:\n",
    "            data = load_json_data(fp)\n",
    "            if isinstance(data, list):\n",
    "                for it in data:\n",
    "                    if isinstance(it, dict):\n",
    "                        it = {**it, \"__source_file__\": os.path.basename(fp)}\n",
    "                        items.append(it)\n",
    "            elif isinstance(data, dict):\n",
    "                data[\"__source_file__\"] = os.path.basename(fp)\n",
    "                items.append(data)\n",
    "            else:\n",
    "                print(f\"Warning: Unsupported JSON root type in {fp}: {type(data)} — skipping\")\n",
    "        print(f\"Total items loaded from directory: {len(items)}\")\n",
    "        return items\n",
    "    elif os.path.isfile(input_path):\n",
    "        print(f\"Loading JSON data from file: {input_path}\")\n",
    "        data = load_json_data(input_path)\n",
    "        if isinstance(data, list):\n",
    "            items = []\n",
    "            for it in data:\n",
    "                if isinstance(it, dict):\n",
    "                    it = {**it, \"__source_file__\": os.path.basename(input_path)}\n",
    "                    items.append(it)\n",
    "            return items\n",
    "        elif isinstance(data, dict):\n",
    "            data[\"__source_file__\"] = os.path.basename(input_path)\n",
    "            return [data]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported JSON root type in {input_path}: {type(data)}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Input path does not exist: {input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurációs változók\n",
    "STEP_PREFIX = '01-acquisition'\n",
    "\n",
    "def save_histogram(series: pd.Series, title: str, path: str, bins: int = 50):\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.hist(series.values, bins=bins)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path)\n",
    "    plt.close(fig)\n",
    "    print(f\"Grafikon mentve: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READABILITY METRICS\n",
    "\n",
    "def count_syllables_hu(word: str) -> int:\n",
    "    \"\"\"Approximate syllable count for Hungarian words based on vowels.\"\"\"\n",
    "    vowels = 'aáeéiíoóöőuúüű'\n",
    "    word = word.lower()\n",
    "    syllable_count = 0\n",
    "    previous_was_vowel = False\n",
    "    for char in word:\n",
    "        is_vowel = char in vowels\n",
    "        if is_vowel and not previous_was_vowel:\n",
    "            syllable_count += 1\n",
    "        previous_was_vowel = is_vowel\n",
    "    return max(1, syllable_count)\n",
    "\n",
    "\n",
    "def flesch_reading_ease_hu(text: str) -> float:\n",
    "    \"\"\"Flesch Reading Ease adapted for Hungarian.\n",
    "    Higher score = easier to read (0-100 scale).\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    \n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    \n",
    "    total_syllables = sum(count_syllables_hu(w) for w in words)\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "    avg_syllables_per_word = total_syllables / len(words)\n",
    "    \n",
    "    score = 206.835 - 1.015 * avg_sentence_length - 84.6 * avg_syllables_per_word\n",
    "    return max(0.0, min(100.0, score))\n",
    "\n",
    "\n",
    "def gunning_fog_index(text: str) -> float:\n",
    "    \"\"\"Gunning Fog Index: years of education needed to understand text.\"\"\"\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    \n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    \n",
    "    complex_words = sum(1 for w in words if count_syllables_hu(w) >= 3)\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "    percent_complex = (complex_words / len(words)) * 100\n",
    "    \n",
    "    fog = 0.4 * (avg_sentence_length + percent_complex)\n",
    "    return fog\n",
    "\n",
    "\n",
    "def smog_index(text: str) -> float:\n",
    "    \"\"\"SMOG Index: years of education needed.\"\"\"\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    if len(sentences) < 3:\n",
    "        return 0.0\n",
    "    \n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    \n",
    "    complex_words = sum(1 for w in words if count_syllables_hu(w) >= 3)\n",
    "    smog = 1.0430 * np.sqrt(complex_words * (30 / len(sentences))) + 3.1291\n",
    "    return smog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64383733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEXICAL DIVERSITY METRICS\n",
    "\n",
    "def type_token_ratio(text: str) -> float:\n",
    "    \"\"\"TTR: unique words / total words.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "\n",
    "def moving_average_ttr(text: str, window_size: int = 100) -> float:\n",
    "    \"\"\"MATTR: average TTR over moving windows.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    if len(words) < window_size:\n",
    "        return type_token_ratio(text)\n",
    "    \n",
    "    ttrs = []\n",
    "    for i in range(len(words) - window_size + 1):\n",
    "        window = words[i:i+window_size]\n",
    "        ttrs.append(len(set(window)) / len(window))\n",
    "    \n",
    "    return np.mean(ttrs) if ttrs else 0.0\n",
    "\n",
    "\n",
    "def hapax_legomena_ratio(text: str) -> float:\n",
    "    \"\"\"Ratio of words that appear only once.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    word_counts = Counter(words)\n",
    "    hapax = sum(1 for count in word_counts.values() if count == 1)\n",
    "    return hapax / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_readability_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add readability metrics to dataframe.\"\"\"\n",
    "    df = df.copy()\n",
    "    texts = df['text_raw'].astype(str)\n",
    "    \n",
    "    print(\"Computing readability metrics...\")\n",
    "    df['flesch_score'] = texts.apply(flesch_reading_ease_hu)\n",
    "    df['fog_index'] = texts.apply(gunning_fog_index)\n",
    "    df['smog_index'] = texts.apply(smog_index)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_lexical_diversity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add lexical diversity metrics to dataframe.\"\"\"\n",
    "    df = df.copy()\n",
    "    texts = df['text_raw'].astype(str)\n",
    "    \n",
    "    print(\"Computing lexical diversity metrics...\")\n",
    "    df['ttr'] = texts.apply(type_token_ratio)\n",
    "    df['mattr'] = texts.apply(moving_average_ttr)\n",
    "    df['hapax_ratio'] = texts.apply(hapax_legomena_ratio)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_by_label(df: pd.DataFrame, features_dir: str):\n",
    "    \"\"\"Plot readability and lexical metrics grouped by label.\"\"\"\n",
    "    if 'label_raw' not in df.columns:\n",
    "        print(\"Skipping label-based plots: no label_raw column\")\n",
    "        return\n",
    "    \n",
    "    metrics = ['flesch_score', 'fog_index', 'smog_index', 'ttr', 'mattr', 'hapax_ratio']\n",
    "    available_metrics = [m for m in metrics if m in df.columns]\n",
    "    \n",
    "    if not available_metrics:\n",
    "        return\n",
    "    \n",
    "    for metric in available_metrics:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        df_plot = df[df[metric].notna()].copy()\n",
    "        df_plot = df_plot[df_plot['label_raw'].notna()].copy()\n",
    "        labels_sorted = sorted([l for l in df_plot['label_raw'].unique() if l is not None])\n",
    "        \n",
    "        if not labels_sorted:\n",
    "            plt.close(fig)\n",
    "            continue\n",
    "        \n",
    "        data_to_plot = [df_plot[df_plot['label_raw'] == label][metric].values \n",
    "                        for label in labels_sorted]\n",
    "        \n",
    "        ax.boxplot(data_to_plot, tick_labels=labels_sorted, patch_artist=True)\n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()} by Label')\n",
    "        ax.set_xlabel('Label')\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        out_path = os.path.join(features_dir, f'{STEP_PREFIX}_{metric}_by_label.png')\n",
    "        fig.savefig(out_path)\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved {metric} by label plot -> {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf_top_words(df: pd.DataFrame, features_dir: str, top_n: int = 20):\n",
    "    \"\"\"Compute and save top TF-IDF words per label.\"\"\"\n",
    "    if 'label_raw' not in df.columns:\n",
    "        print(\"Skipping TF-IDF: no label_raw column\")\n",
    "        return\n",
    "    \n",
    "    print(\"Computing TF-IDF top words per label...\")\n",
    "    \n",
    "    labels = [l for l in df['label_raw'].unique() if l is not None]\n",
    "    results = []\n",
    "    \n",
    "    for label in sorted(labels):\n",
    "        label_texts = df[df['label_raw'] == label]['text_raw'].astype(str).tolist()\n",
    "        if not label_texts:\n",
    "            continue\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2), \n",
    "                                     min_df=2, max_df=0.8)\n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform(label_texts)\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            avg_scores = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "            top_indices = avg_scores.argsort()[-top_n:][::-1]\n",
    "            \n",
    "            top_words = [(feature_names[i], avg_scores[i]) for i in top_indices]\n",
    "            results.append({\n",
    "                'label': label,\n",
    "                'top_words': ', '.join([f\"{word}({score:.3f})\" for word, score in top_words[:10]])\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"TF-IDF failed for label {label}: {e}\")\n",
    "    \n",
    "    if results:\n",
    "        df_tfidf = pd.DataFrame(results)\n",
    "        out_csv = os.path.join(features_dir, f'{STEP_PREFIX}_tfidf_top_words_by_label.csv')\n",
    "        df_tfidf.to_csv(out_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Saved TF-IDF results -> {out_csv}\")\n",
    "        display(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(df: pd.DataFrame, features_dir: str):\n",
    "    \"\"\"Plot correlation heatmap of numerical features including label.\"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if 'label_raw' in df.columns:\n",
    "        df_corr = df.copy()\n",
    "        df_corr['label_numeric'] = df_corr['label_raw'].astype(str).str.extract(r'(\\d+)')[0].astype(float)\n",
    "        numeric_cols.append('label_numeric')\n",
    "    else:\n",
    "        df_corr = df.copy()\n",
    "    \n",
    "    relevant = [col for col in numeric_cols if col in df_corr.columns]\n",
    "    if len(relevant) < 2:\n",
    "        print(\"Not enough numeric columns for correlation matrix\")\n",
    "        return\n",
    "    \n",
    "    corr_matrix = df_corr[relevant].corr()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "               center=0, ax=ax, square=True, linewidths=0.5)\n",
    "    \n",
    "    ax.set_title('Feature Correlation Matrix')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    out_path = os.path.join(features_dir, f'{STEP_PREFIX}_correlation_matrix.png')\n",
    "    fig.savefig(out_path, dpi=100)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved correlation matrix -> {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8689f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_confusion_pairs(df: pd.DataFrame, features_dir: str):\n",
    "    \"\"\"Plot top confusion pairs: szomszédos címkék közötti potenciális összekeverés.\"\"\"\n",
    "    if 'label_raw' not in df.columns:\n",
    "        print(\"Skipping confusion pairs: no label_raw column\")\n",
    "        return\n",
    "    \n",
    "    df_analysis = df.copy()\n",
    "    df_analysis['label_numeric'] = df_analysis['label_raw'].astype(str).str.extract(r'(\\d+)')[0].astype(float)\n",
    "    \n",
    "    label_pairs = []\n",
    "    for label_num in sorted(df_analysis['label_numeric'].dropna().unique()):\n",
    "        neighbor = label_num + 1\n",
    "        if neighbor in df_analysis['label_numeric'].values:\n",
    "            label_1 = df_analysis[df_analysis['label_numeric'] == label_num]['label_raw'].iloc[0]\n",
    "            label_2 = df_analysis[df_analysis['label_numeric'] == neighbor]['label_raw'].iloc[0]\n",
    "            count_1 = len(df_analysis[df_analysis['label_numeric'] == label_num])\n",
    "            count_2 = len(df_analysis[df_analysis['label_numeric'] == neighbor])\n",
    "            avg_count = (count_1 + count_2) / 2\n",
    "            label_pairs.append((f\"{label_1} ↔ {label_2}\", avg_count))\n",
    "    \n",
    "    if not label_pairs:\n",
    "        print(\"No confusion pairs to plot\")\n",
    "        return\n",
    "    \n",
    "    label_pairs = sorted(label_pairs, key=lambda x: x[1], reverse=True)[:5]\n",
    "    pair_names, pair_counts = zip(*label_pairs)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.barh(pair_names, pair_counts, color='#F58518')\n",
    "    ax.set_xlabel('Átlagos mintaszám (potenciális összekeverés)')\n",
    "    ax.set_title('Top Confusion Pairs (szomszédos címkék)')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    out_path = os.path.join(features_dir, f'{STEP_PREFIX}_top_confusion_pairs.png')\n",
    "    fig.savefig(out_path)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved top confusion pairs -> {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b88c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_eda(df: pd.DataFrame, features_dir: str):\n",
    "    \"\"\"Compute and save simple EDA plots on RAW text (no cleaning).\"\"\"\n",
    "    texts = df.get('text_raw', pd.Series([], dtype=str)).astype(str)\n",
    "    word_counts = texts.apply(lambda t: len(t.split()))\n",
    "    avg_word_len = texts.apply(lambda t: np.mean([len(w) for w in t.split()]) if t.split() else 0.0)\n",
    "\n",
    "    Path(features_dir).mkdir(parents=True, exist_ok=True)\n",
    "    save_histogram(word_counts, 'RAW Word Count Distribution', \n",
    "                   os.path.join(features_dir, f'{STEP_PREFIX}_raw_word_count_hist.png'))\n",
    "    save_histogram(avg_word_len, 'RAW Average Word Length Distribution', \n",
    "                   os.path.join(features_dir, f'{STEP_PREFIX}_raw_avg_word_len_hist.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_from_annotations(annotations_raw: str):\n",
    "    \"\"\"Extract label from Label Studio annotations JSON.\"\"\"\n",
    "    if not annotations_raw:\n",
    "        return None\n",
    "    try:\n",
    "        annotations = json.loads(annotations_raw)\n",
    "        if annotations and isinstance(annotations, list) and len(annotations) > 0:\n",
    "            return annotations[0]['result'][0]['value']['choices'][0]\n",
    "    except (KeyError, IndexError, TypeError, json.JSONDecodeError):\n",
    "        return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda_label_analysis(df: pd.DataFrame, raw_dir: str, features_dir: str):\n",
    "    \"\"\"Perform EDA-only label checks.\"\"\"\n",
    "    if 'text_raw' not in df.columns:\n",
    "        print(\"EDA label analysis skipped: 'text_raw' column missing.\")\n",
    "        return\n",
    "\n",
    "    df = df.copy()\n",
    "    df['label_raw'] = df.get('annotations_raw', '').apply(extract_label_from_annotations)\n",
    "\n",
    "    total_rows = len(df)\n",
    "    missing_mask = (df['label_raw'].isna() | (df['label_raw'] == '')) | (df['text_raw'].isna() | (df['text_raw'] == ''))\n",
    "    missing_count = int(missing_mask.sum())\n",
    "\n",
    "    print(f\"EDA label analysis: rows={total_rows}, missing_label_or_text={missing_count}\")\n",
    "\n",
    "    # Save statistics\n",
    "    stats_file = os.path.join(features_dir, f'{STEP_PREFIX}_raw_eda_statistics.txt')\n",
    "    with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"RAW EDA Statisztikák\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Összes sor: {total_rows}\\n\")\n",
    "        f.write(f\"Hiányzó label VAGY üres text: {missing_count}\\n\")\n",
    "    print(f\"Saved EDA statistics -> {stats_file}\")\n",
    "\n",
    "    # Plot label distribution\n",
    "    try:\n",
    "        counts = df['label_raw'].value_counts().sort_values(ascending=False)\n",
    "        if len(counts) == 0:\n",
    "            print(\"No labels available for distribution plot.\")\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(8, 4))\n",
    "            counts.plot(kind='bar', ax=ax, color='#4C78A8')\n",
    "            ax.set_title('Besorolás eloszlás (RAW EDA)')\n",
    "            ax.set_xlabel('Besorolás (label)')\n",
    "            ax.set_ylabel('Darab')\n",
    "            fig.tight_layout()\n",
    "            out_fig = os.path.join(features_dir, f'{STEP_PREFIX}_raw_label_distribution.png')\n",
    "            fig.savefig(out_fig)\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "            print(f\"Saved label distribution plot -> {out_fig}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate label distribution plot: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ADVANCED STATISTICAL ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Compute metrics\n",
    "    try:\n",
    "        df = compute_readability_metrics(df)\n",
    "        print(\"✓ Readability metrics computed\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Readability metrics failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        df = compute_lexical_diversity(df)\n",
    "        print(\"✓ Lexical diversity metrics computed\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Lexical diversity failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        plot_metrics_by_label(df, features_dir)\n",
    "        print(\"✓ Metrics by label plots saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Metrics by label plots failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        compute_tfidf_top_words(df, features_dir)\n",
    "        print(\"✓ TF-IDF analysis completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ TF-IDF analysis failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        plot_correlation_matrix(df, features_dir)\n",
    "        print(\"✓ Correlation matrix saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Correlation matrix failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        plot_top_confusion_pairs(df, features_dir)\n",
    "        print(\"✓ Top confusion pairs plot saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Top confusion pairs failed: {e}\")\n",
    "    \n",
    "    # Save enhanced dataset\n",
    "    try:\n",
    "        out_csv_enhanced = os.path.join(raw_dir, 'raw_dataset_eda_enhanced.csv')\n",
    "        df_enhanced = df.drop(columns=['row_id'], errors='ignore')\n",
    "        df_enhanced.to_csv(out_csv_enhanced, index=False, encoding='utf-8-sig')\n",
    "        print(f\"✓ Saved enhanced EDA dataset -> {out_csv_enhanced}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to save enhanced dataset: {e}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"ADVANCED ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cf886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_data(input_path: str, raw_dir: str, features_dir: str):\n",
    "    \"\"\"Aggregate RAW JSON items and persist a lightweight snapshot.\"\"\"\n",
    "    Path(raw_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(features_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Loading JSON data...\")\n",
    "    data_items = load_json_items(input_path)\n",
    "\n",
    "    records = []\n",
    "    for item in data_items:\n",
    "        text = ''\n",
    "        annotations = None\n",
    "        source = ''\n",
    "        if isinstance(item, dict):\n",
    "            text = item.get('data', {}).get('text', '')\n",
    "            annotations = item.get('annotations')\n",
    "            source = item.get('__source_file__', '')\n",
    "        records.append({\n",
    "            'text_raw': text if text is not None else '',\n",
    "            'annotations_raw': json.dumps(annotations, ensure_ascii=False) if annotations is not None else '',\n",
    "            'source_file': source\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"RAW rows aggregated: {len(df)}\")\n",
    "\n",
    "    raw_csv = os.path.join(raw_dir, 'raw_dataset.csv')\n",
    "    df.to_csv(raw_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved RAW dataset -> {raw_csv}\")\n",
    "\n",
    "    # RAW EDA\n",
    "    try:\n",
    "        raw_eda(df, features_dir)\n",
    "        print(f\"Saved RAW EDA plots to {features_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"RAW EDA failed (continuing): {e}\")\n",
    "\n",
    "    # Additional EDA\n",
    "    try:\n",
    "        df = eda_label_analysis(df, raw_dir, features_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"EDA label analysis failed (continuing): {e}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897479b",
   "metadata": {},
   "source": [
    "## Adatok betöltése és elemzése\n",
    "\n",
    "Most futtatjuk a pipeline-t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add9285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurációs változók\n",
    "data_dir = os.getenv('DATA_DIR', '../data')\n",
    "base_output = os.getenv('OUTPUT_DIR', '../output')\n",
    "\n",
    "raw_dir = os.path.join(base_output, 'raw')\n",
    "features_dir = os.path.join(base_output, 'reports')\n",
    "\n",
    "print(f\"Input path: {data_dir}\")\n",
    "print(f\"RAW output: {raw_dir}\")\n",
    "\n",
    "# Ensure data is available\n",
    "if not ensure_data_available(data_dir):\n",
    "    print(\"ERROR: No data available. Exiting.\")\n",
    "else:\n",
    "    # Process data\n",
    "    df_result = process_raw_data(data_dir, raw_dir, features_dir)\n",
    "    print(\"\\nData processing complete!\")\n",
    "    display(df_result.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
