{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39addb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import könyvtárak\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurációs változók\n",
    "STEP_PREFIX = '02-preparation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text data for Hungarian legal texts (lowercase, normalize, strip).\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s\\.,!\\?;:\\-–—\\(\\)\"\\'„\"%/€$…]', '', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c693bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(df, target_column, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"Split data into train/validation/test sets with stratification.\"\"\"\n",
    "    train_val, test = train_test_split(\n",
    "        df, test_size=test_size, stratify=df[target_column], random_state=random_state\n",
    "    )\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    train, val = train_test_split(\n",
    "        train_val, test_size=val_size_adjusted, stratify=train_val[target_column], random_state=random_state\n",
    "    )\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a16cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add basic text stats: word_count, avg_word_len.\"\"\"\n",
    "    if 'text' not in df.columns:\n",
    "        return df\n",
    "    texts = df['text'].astype(str)\n",
    "    df = df.copy()\n",
    "    df['word_count'] = texts.apply(lambda t: len(t.split()))\n",
    "    df['avg_word_len'] = texts.apply(lambda t: np.mean([len(w) for w in t.split()]) if t.split() else 0.0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_histogram(series: pd.Series, title: str, path: str, bins: int = 50):\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.hist(series.values, bins=bins)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    print(f\"Grafikon mentve: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d666504",
   "metadata": {},
   "source": [
    "## Adatok betöltése és tisztítása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18132c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguráció\n",
    "base_output = os.getenv('OUTPUT_DIR', '../output')\n",
    "raw_dir = os.path.join(base_output, 'raw')\n",
    "processed_dir = os.path.join(base_output, 'processed')\n",
    "features_dir = os.path.join(base_output, 'reports')\n",
    "\n",
    "Path(processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(features_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"RAW input: {raw_dir}\")\n",
    "print(f\"Processed output: {processed_dir}\")\n",
    "print(f\"Reports dir: {features_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw dataset\n",
    "raw_csv = os.path.join(raw_dir, 'raw_dataset.csv')\n",
    "if not os.path.exists(raw_csv):\n",
    "    raise FileNotFoundError(f\"Missing {raw_csv}. Run 01_data_acquisition_and_analysis.py first.\")\n",
    "\n",
    "df_raw = pd.read_csv(raw_csv)\n",
    "print(f\"Loaded {len(df_raw)} rows from {raw_csv}\")\n",
    "display(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120feac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract label from annotations\n",
    "def extract_label_from_annotations(annotations_raw):\n",
    "    if not annotations_raw:\n",
    "        return None\n",
    "    try:\n",
    "        annotations = json.loads(annotations_raw)\n",
    "        if annotations and isinstance(annotations, list) and len(annotations) > 0:\n",
    "            return annotations[0]['result'][0]['value']['choices'][0]\n",
    "    except (KeyError, IndexError, TypeError, json.JSONDecodeError):\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "if 'label_raw' not in df_raw.columns:\n",
    "    df_raw['label_raw'] = df_raw.get('annotations_raw', '').apply(extract_label_from_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76815c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build working DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': df_raw['text_raw'],\n",
    "    'label': df_raw['label_raw']\n",
    "})\n",
    "\n",
    "print(f\"Rows before cleaning: {len(df)}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d619a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "print(\"Text cleaning complete\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd6fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty text or label\n",
    "before = len(df)\n",
    "missing_mask = (df['text'].isna() | (df['text'] == '')) | (df['label'].isna() | (df['label'] == ''))\n",
    "missing_rows = df[missing_mask].copy()\n",
    "df = df[~missing_mask].reset_index(drop=True)\n",
    "after_missing = len(df)\n",
    "\n",
    "print(f\"Rows after removing empty text/label: {after_missing} (removed {before - after_missing})\")\n",
    "\n",
    "if before - after_missing > 0:\n",
    "    missing_file = os.path.join(raw_dir, f'{STEP_PREFIX}_removed_missing_labels_or_text.csv')\n",
    "    missing_rows.to_csv(missing_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved removed missing rows to {missing_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe61de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate by lowercase-cleaned text\n",
    "df['text_dedup'] = df['text'].str.lower()\n",
    "before_dedup = len(df)\n",
    "df = df.drop_duplicates(subset=['text_dedup'], keep='first').reset_index(drop=True)\n",
    "after_dedup = len(df)\n",
    "\n",
    "print(f\"Rows after deduplication: {after_dedup} (removed {before_dedup - after_dedup})\")\n",
    "\n",
    "if before_dedup - after_dedup > 0:\n",
    "    dup_file = os.path.join(raw_dir, f'{STEP_PREFIX}_removed_duplicates.csv')\n",
    "    deduped = df['text_dedup'].tolist()\n",
    "    dropped = df_raw[~df_raw['text_raw'].str.lower().isin(deduped)]\n",
    "    dropped.to_csv(dup_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved removed duplicates to {dup_file}\")\n",
    "\n",
    "# Drop helper column\n",
    "df = df.drop(columns=['text_dedup'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edafca6",
   "metadata": {},
   "source": [
    "## Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split\n",
    "target_column = 'label'\n",
    "if target_column in df.columns and len(df) > 10:\n",
    "    print(\"Performing stratified split...\")\n",
    "    train_df, val_df, test_df = stratified_split(df, target_column)\n",
    "    print(f\"Train set: {len(train_df)}, Val set: {len(val_df)}, Test set: {len(test_df)}\")\n",
    "    \n",
    "    # Display distribution\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    print(\"Train:\")\n",
    "    print(train_df['label'].value_counts())\n",
    "    print(\"\\nValidation:\")\n",
    "    print(val_df['label'].value_counts())\n",
    "    print(\"\\nTest:\")\n",
    "    print(test_df['label'].value_counts())\n",
    "else:\n",
    "    print(f\"Insufficient data for stratified split.\")\n",
    "    train_df = df\n",
    "    val_df = None\n",
    "    test_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text stats to each split\n",
    "for name, split_df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
    "    if split_df is None:\n",
    "        continue\n",
    "    aug = add_text_stats(split_df)\n",
    "    out_csv = os.path.join(processed_dir, f'{name}.csv')\n",
    "    aug.to_csv(out_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved {name}.csv with stats -> {out_csv}\")\n",
    "    display(aug.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c58e0",
   "metadata": {},
   "source": [
    "## Clean EDA Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117c124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean EDA histograms (from train split)\n",
    "if len(train_df) > 0 and 'text' in train_df.columns:\n",
    "    temp = add_text_stats(train_df)\n",
    "    \n",
    "    if 'word_count' in temp.columns:\n",
    "        save_histogram(temp['word_count'], 'CLEAN Word Count Distribution (Train)',\n",
    "                       os.path.join(features_dir, f'{STEP_PREFIX}_clean_word_count_hist.png'))\n",
    "    \n",
    "    if 'avg_word_len' in temp.columns:\n",
    "        save_histogram(temp['avg_word_len'], 'CLEAN Average Word Length Distribution (Train)',\n",
    "                       os.path.join(features_dir, f'{STEP_PREFIX}_clean_avg_word_len_hist.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa47761",
   "metadata": {},
   "source": [
    "## Adatelőkészítés befejezve!\n",
    "\n",
    "Az adatok tisztítva, train/val/test split-re osztva és elmentve."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
