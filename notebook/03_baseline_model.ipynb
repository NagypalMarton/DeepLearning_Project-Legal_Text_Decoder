{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4751d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import könyvtárak\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d20303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_csv(processed_dir: str):\n",
    "    \"\"\"Load train/val/test CSVs from processed_dir. Returns (train, val, test).\"\"\"\n",
    "    train_path = os.path.join(processed_dir, \"train.csv\")\n",
    "    val_path = os.path.join(processed_dir, \"val.csv\")\n",
    "    test_path = os.path.join(processed_dir, \"test.csv\")\n",
    "\n",
    "    if not os.path.exists(train_path):\n",
    "        single_path = os.path.join(processed_dir, \"processed_data.csv\")\n",
    "        if os.path.exists(single_path):\n",
    "            df = pd.read_csv(single_path)\n",
    "            return df, None, None\n",
    "        raise FileNotFoundError(f\"Missing training data in {processed_dir}\")\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    val_df = pd.read_csv(val_path) if os.path.exists(val_path) else None\n",
    "    test_df = pd.read_csv(test_path) if os.path.exists(test_path) else None\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bef347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels, save_path, split_name='Test'):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=labels, yticklabels=labels,\n",
    "           ylabel='True label', xlabel='Predicted label',\n",
    "           title=f'Confusion Matrix ({split_name})')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    thresh = cm.max() / 2.0 if cm.size else 0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'), ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_numeric(labels):\n",
    "    numeric = []\n",
    "    for label in labels:\n",
    "        s = str(label).strip()\n",
    "        if s and s[0].isdigit():\n",
    "            numeric.append(int(s[0]))\n",
    "        else:\n",
    "            try:\n",
    "                numeric.append(int(s))\n",
    "            except ValueError:\n",
    "                numeric.append(0)\n",
    "    return np.array(numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edea57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineTransformerDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=320):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        enc = self.tokenizer(text,\n",
    "                             add_special_tokens=True,\n",
    "                             max_length=self.max_length,\n",
    "                             padding='max_length',\n",
    "                             truncation=True,\n",
    "                             return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28409dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_mapping(labels):\n",
    "    unique = sorted(list(set(labels)), key=lambda x: int(str(x)[0]) if str(x)[0].isdigit() else x)\n",
    "    label2id = {lab: idx for idx, lab in enumerate(unique)}\n",
    "    id2label = {idx: lab for lab, idx in label2id.items()}\n",
    "    return label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, scheduler, device, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    disable_tqdm = not sys.stdout.isatty()\n",
    "    for batch in tqdm(dataloader, desc=\"Train\", disable=disable_tqdm):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        running_loss += loss.item() * input_ids.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss / max(1, total), correct / max(1, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device, criterion, id2label):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "    disable_tqdm = not sys.stdout.isatty()\n",
    "    for batch in tqdm(dataloader, desc=\"Eval\", disable=disable_tqdm):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        running_loss += loss.item() * input_ids.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_preds.extend([id2label[int(p)] for p in preds.cpu().numpy()])\n",
    "        all_trues.extend([id2label[int(t)] for t in labels.cpu().numpy()])\n",
    "    avg_loss = running_loss / max(1, total)\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc, all_preds, all_trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ae13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_overfitting_convergence(losses, accuracies, final_iteration, reports_dir):\n",
    "    \"\"\"Plot Loss and Accuracy curves for overfitting test convergence.\"\"\"\n",
    "    iterations = range(1, len(losses) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(iterations, losses, linewidth=2, color='#e74c3c', alpha=0.8)\n",
    "    ax1.axhline(y=0.001, color='green', linestyle='--', linewidth=2, label='Target (loss < 0.001)', alpha=0.7)\n",
    "    ax1.set_xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Overfitting Test: Loss Convergence', fontsize=13, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(iterations, accuracies, linewidth=2, color='#27ae60', alpha=0.8)\n",
    "    ax2.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Target (acc = 100%)', alpha=0.7)\n",
    "    ax2.set_xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Overfitting Test: Accuracy Convergence', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylim([0, 1.1])\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax2.legend(fontsize=11)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(reports_dir, '03-baseline_overfitting_test_convergence.png')\n",
    "    fig.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"Saved overfitting test convergence plot to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a6d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfitting_test(model, tokenizer, device, X_data, y_data_ids, label2id, id2label, max_length=320, max_iterations=1000, reports_dir='../output/reports'):\n",
    "    \"\"\"Overfitting test: train on a single batch (32 samples) until 100% accuracy and loss < 0.001.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OVERFITTING TEST: Training on single batch (32 samples)\")\n",
    "    print(\"Target: 100% accuracy and loss < 0.001\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Take only first 32 samples\n",
    "    test_batch_size = min(32, len(X_data))\n",
    "    X_test = X_data[:test_batch_size]\n",
    "    y_test = y_data_ids[:test_batch_size]\n",
    "    \n",
    "    print(f\"Batch size: {test_batch_size}\")\n",
    "    print(f\"Unique labels in batch: {set(y_test)}\")\n",
    "    \n",
    "    # Create dataset\n",
    "    test_ds = BaselineTransformerDataset(X_test, y_test, tokenizer, max_length=max_length)\n",
    "    test_loader = DataLoader(test_ds, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Disable regularization\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.p = 0.0\n",
    "    \n",
    "    # Optimizer with NO weight decay\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "    \n",
    "    model.train()\n",
    "    iteration = 0\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Track metrics for visualization\n",
    "    history_losses = []\n",
    "    history_accuracies = []\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        \n",
    "        for batch in test_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            accuracy = (preds == labels).sum().item() / len(labels)\n",
    "            \n",
    "            # Store metrics for plotting\n",
    "            history_losses.append(loss.item())\n",
    "            history_accuracies.append(accuracy)\n",
    "            \n",
    "            if iteration % 50 == 0 or iteration == 1:\n",
    "                print(f\"Iteration {iteration:4d} | Loss: {loss.item():.6f} | Accuracy: {accuracy:.4f} ({int(accuracy*100)}%)\")\n",
    "            \n",
    "            # Check success criteria\n",
    "            if loss.item() < 0.001 and accuracy == 1.0:\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"SUCCESS! Overfitting test passed!\")\n",
    "                print(f\"Final Loss: {loss.item():.8f} | Final Accuracy: {accuracy:.4f} (100%)\")\n",
    "                print(f\"Converged in {iteration} iterations\")\n",
    "                print(\"=\"*80 + \"\\n\")\n",
    "                \n",
    "                # Plot convergence curve\n",
    "                _plot_overfitting_convergence(history_losses, history_accuracies, iteration, reports_dir)\n",
    "                \n",
    "                return True\n",
    "            \n",
    "            # Check for improvement\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "    \n",
    "    # If we reach here, test failed\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FAILURE! Overfitting test did NOT converge to 100% accuracy + loss < 0.001\")\n",
    "    print(f\"Reached iteration {iteration} without meeting criteria\")\n",
    "    print(f\"Best loss achieved: {best_loss:.6f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    raise RuntimeError(\"OVERFITTING TEST FAILED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6511c",
   "metadata": {},
   "source": [
    "## Model Training & Overfitting Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "base_output = os.getenv('OUTPUT_DIR', '../output')\n",
    "processed_dir = os.path.join(base_output, 'processed')\n",
    "models_dir = os.path.join(base_output, 'models')\n",
    "reports_dir = os.path.join(base_output, 'reports')\n",
    "\n",
    "Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(reports_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Processed data dir: {processed_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df, val_df, test_df = load_split_csv(processed_dir)\n",
    "if 'text' not in train_df.columns or 'label' not in train_df.columns:\n",
    "    raise ValueError(\"Train CSV must contain 'text' and 'label'\")\n",
    "\n",
    "X_train = train_df['text'].astype(str).tolist()\n",
    "y_train = train_df['label'].astype(str).tolist()\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f26797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping\n",
    "label2id, id2label = create_label_mapping(y_train)\n",
    "mapping_path = os.path.join(models_dir, 'baseline_label_mapping.json')\n",
    "with open(mapping_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump({'label2id': label2id, 'id2label': {str(k): v for k, v in id2label.items()}}, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved baseline label mapping to {mapping_path}\")\n",
    "\n",
    "y_train_ids = [label2id[l] for l in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "model_name = os.getenv('BASELINE_TRANSFORMER_MODEL', os.getenv('TRANSFORMER_MODEL', 'SZTAKI-HLT/hubert-base-cc'))\n",
    "batch_size = int(os.getenv('BATCH_SIZE', '8'))\n",
    "max_length = int(os.getenv('BASELINE_MAX_LENGTH', os.getenv('MAX_LENGTH', '320')))\n",
    "\n",
    "print(f\"Baseline Transformer: {model_name}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Max length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28090dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id))\n",
    "model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model Architecture: {total_params:,} total | {trainable_params:,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN OVERFITTING TEST\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE MODEL: Overfitting Test Mode\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "overfitting_test(model, tokenizer, device, X_train, y_train_ids, label2id, id2label, max_length=max_length, reports_dir=reports_dir)\n",
    "print(\"Overfitting test PASSED! Model validated successfully.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f829b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline transformer model after successful overfitting test\n",
    "baseline_model_dir = os.path.join(models_dir, 'baseline_transformer_model')\n",
    "Path(baseline_model_dir).mkdir(parents=True, exist_ok=True)\n",
    "model.save_pretrained(baseline_model_dir)\n",
    "tokenizer.save_pretrained(baseline_model_dir)\n",
    "print(f\"Model saved to {baseline_model_dir}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
