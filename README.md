# Legal Text Decoder# Legal Text Decoder



NLP rendszer jogi szÃ¶vegek (ÃSZF/ÃFF) Ã©rthetÅ‘sÃ©gÃ©nek automatikus Ã©rtÃ©kelÃ©sÃ©re (1-5 skÃ¡la).  NLP rendszer jogi szÃ¶vegek (ÃSZF/ÃFF) Ã©rthetÅ‘sÃ©gÃ©nek automatikus Ã©rtÃ©kelÃ©sÃ©re (1-5 skÃ¡la). Docker + PyTorch + GPU tÃ¡mogatÃ¡s.

**Docker + PyTorch + GPU tÃ¡mogatÃ¡s | Cross-platform**

## ğŸ“š TartalomjegyzÃ©k

---

- [Gyors IndÃ­tÃ¡s](#-gyors-indÃ­tÃ¡s)

## ğŸš€ Gyors IndÃ­tÃ¡s- [KÃ¶vetelmÃ©ny-FÃ¡jl MegfeleltetÃ©s](#-kÃ¶vetelmÃ©ny-fÃ¡jl-megfeleltetÃ©s)

- [FÅ‘ lÃ©pÃ©sek (pipeline)](#-fÅ‘-lÃ©pÃ©sek-pipeline)

### LegegyszerÅ±bb mÃ³dszer (automatikus platform detektÃ¡lÃ¡s):- [AdatformÃ¡tum](#-adatformÃ¡tum)

- [KÃ¶rnyezeti vÃ¡ltozÃ³k](#-kÃ¶rnyezeti-vÃ¡ltozÃ³k)

```bash- [Kimenetek](#-kimenetek)

# Windows PowerShell- [ML Service - API + GUI](#-ml-service---api--gui)

.\docker-run.ps1- [HibaelhÃ¡rÃ­tÃ¡s](#-hibaelhÃ¡rÃ­tÃ¡s)



# Linux/macOS/Git Bash## ğŸ¯ KÃ¶vetelmÃ©ny-FÃ¡jl MegfeleltetÃ©s

bash docker-run.sh

```| # | Outstanding Level KÃ¶vetelmÃ©ny | ImplementÃ¡ciÃ³ | FÃ¡jl |

|---|-------------------------------|---------------|------|

### ManuÃ¡lis Docker futtatÃ¡s:| 1 | **Containerization** | Docker + GPU tÃ¡mogatÃ¡s | `Dockerfile` |

| 2 | **Data acquisition and analysis** | JSON parser, EDA, statistikÃ¡k | `01_data_acquisition_and_analysis.py` |

```bash| 3 | **Data cleansing and preparation** | Text cleaning, stratified split | `02_data_cleansing_and_preparation.py` |

# 1. Build| 4 | **Defining evaluation criteria** | Metrics, confusion matrix | `05_defining_evaluation_criteria.py` |

docker build -t legal-text-decoder:1.0 .| 5 | **Baseline model** | TF-IDF + LogisticRegression | `03_baseline_model.py` |

| 6 | **Incremental model development** | Transformer (HuBERT) fine-tuning | `04_incremental_model_development.py` |

# 2. FuttatÃ¡s (Windows PowerShell)| 7 | **Advanced evaluation** | Robustness + Explainability | `06_advanced_evaluation_robustness.py` <br> `07_advanced_evaluation_explainability.py` |

docker run --rm --gpus all `| 8 | **ML as a service** | REST API + Web GUI | `src/api/app.py` <br> `src/frontend/app.py` |

  -v "${PWD}\data:/app/data" `

  -v "${PWD}\output:/app/output" `## ğŸ“‹ FÅ‘ lÃ©pÃ©sek (pipeline)

  legal-text-decoder:1.0

1. **01_data_acquisition_and_analysis.py** â€” JSON adatok betÃ¶ltÃ©se (fÃ¡jl vagy mappa), szÃ¶veg tisztÃ­tÃ¡s, label kinyerÃ©s, stratifikÃ¡lt train/val/test split Ã©s mentÃ©s CSV-be az OUTPUT_DIR/processed mappÃ¡ba.

# 2. FuttatÃ¡s (Linux/macOS)2. **02_data_cleansing_and_preparation.py** â€” EgyszerÅ± szÃ¶vegstatisztikÃ¡k (word_count, avg_word_len) hozzÃ¡adÃ¡sa Ã©s opcionÃ¡lis Sentence-BERT beÃ¡gyazÃ¡sok mentÃ©se az OUTPUT_DIR/features mappÃ¡ba.

docker run --rm --gpus all \3. **03_baseline_model.py** â€” Baseline szÃ¶vegklasszifikÃ¡ciÃ³s modell: TFâ€‘IDF + LogisticRegression. Modell mentÃ©se (OUTPUT_DIR/models), metrikÃ¡k mentÃ©se (OUTPUT_DIR/reports).

  -v "$(pwd)/data:/app/data" \4. **04_incremental_model_development.py** â€” Transformer alapÃº modell (pl. HuBERT) finomhangolÃ¡sa a jogi szÃ¶vegeken. GPU ajÃ¡nlott! Modell Ã©s tokenizer mentÃ©se (OUTPUT_DIR/models/transformer_model).

  -v "$(pwd)/output:/app/output" \5. **05_defining_evaluation_criteria.py** â€” KÃ¼lÃ¶n Ã©rtÃ©kelÅ‘ script a baseline modellre a test spliten (OUTPUT_DIR/evaluation).

  legal-text-decoder:1.06. **06_advanced_evaluation_robustness.py** â€” RobusztussÃ¡gi tesztek: zajjal Ã©s csonkolÃ¡ssal mÃ³dosÃ­tott szÃ¶vegeken Ã©rtÃ©keli a baseline modellt (OUTPUT_DIR/robustness).

```7. **07_advanced_evaluation_explainability.py** â€” Modell Ã©rtelmezhetÅ‘sÃ©g: top feature-Ã¶k osztÃ¡lyonkÃ©nt, predikciÃ³ magyarÃ¡zatok, hibaelemzÃ©s (OUTPUT_DIR/explainability).



**FutÃ¡si idÅ‘:** ~45-60 perc GPU-val | ~6+ Ã³ra CPU-n> A `src/run.sh` sorban futtatja az Ã¶sszes `src/*.py` fÃ¡jlt (Ã¡bÃ©cÃ©rendben). Dockerben ez az alapÃ©rtelmezett belÃ©pÃ©si pont.



---## AdatformÃ¡tum (JSON)



## ğŸ¯ KÃ¶vetelmÃ©ny-FÃ¡jl MegfeleltetÃ©sElvÃ¡rt minimÃ¡lis sÃ©ma egy elemre:



| # | KÃ¶vetelmÃ©ny | FÃ¡jl |```json

|---|-------------|------|{

| 1 | **Containerization** | `Dockerfile` |	"data": { "text": "A bekezdÃ©s szÃ¶vegeâ€¦" },

| 2 | **Data acquisition** | `01_data_acquisition_and_analysis.py` |	"annotations": [

| 3 | **Data cleansing** | `02_data_cleansing_and_preparation.py` |		{

| 4 | **Evaluation criteria** | `05_defining_evaluation_criteria.py` |			"result": [

| 5 | **Baseline model** | `03_baseline_model.py` (TF-IDF + LogReg) |				{ "value": { "choices": ["KÃ¶nnyen Ã©rthetÅ‘"] } }

| 6 | **Incremental development** | `04_incremental_model_development.py` (HuBERT) |			]

| 7 | **Advanced evaluation** | `06_advanced_evaluation_robustness.py`<br>`07_advanced_evaluation_explainability.py` |		}

| 8 | **ML as a service** | `src/api/app.py` (FastAPI)<br>`src/frontend/app.py` (Streamlit) |	]

}

---```



## ğŸ“‹ Pipeline (7 lÃ©pÃ©s)Fontos: ha tÃ¶bb annotÃ¡ciÃ³/eredmÃ©ny van, jelenleg az elsÅ‘ elem elsÅ‘ vÃ¡lasztÃ¡sa kerÃ¼l felhasznÃ¡lÃ¡sra.



1. **Data Acquisition** - JSON betÃ¶ltÃ©s, tisztÃ­tÃ¡s, stratified split (60/20/20)## KÃ¶rnyezeti vÃ¡ltozÃ³k

2. **Data Preparation** - SzÃ¶vegstatisztikÃ¡k, opcionÃ¡lis embeddings

3. **Baseline Model** - TF-IDF + LogisticRegression tanÃ­tÃ¡s**AdatkezelÃ©s:**

4. **Transformer Model** - HuBERT fine-tuning (GPU ajÃ¡nlott!)- `DATA_DIR` â€” Bemeneti adat mappa (alap: `/app/data` Dockerben).

5. **Evaluation** - Test set Ã©rtÃ©kelÃ©s, confusion matrix- `OUTPUT_DIR` â€” Kimeneti mappa (alap: `/app/output`).

6. **Robustness** - Zajjal Ã©s csonkolÃ¡ssal tesztelÃ©s

7. **Explainability** - Feature importance, hibaelemzÃ©s**Baseline modell (TF-IDF + LogisticRegression):**

- `TFIDF_MAX_FEATURES` â€” TFâ€‘IDF max jellemzÅ‘k szÃ¡ma (alap: 20000).

> Pipeline orchestrator: `src/run.sh` (automatikusan fut Docker-ben)- `TFIDF_NGRAM_MAX` â€” TFâ€‘IDF n-gram felsÅ‘ hatÃ¡r (alap: 2).

- `LR_C` â€” LogisticRegression C paramÃ©tere (alap: 1.0).

---

**Transformer modell:**

## ğŸ“„ AdatformÃ¡tum (JSON)- `TRANSFORMER_MODEL` â€” HasznÃ¡landÃ³ transformer modell neve (alap: `SZTAKI-HLT/hubert-base-cc`).

- `BATCH_SIZE` â€” Batch mÃ©ret a tanÃ­tÃ¡shoz (alap: 8).

```json- `EPOCHS` â€” TanÃ­tÃ¡si epochok szÃ¡ma (alap: 3).

{- `LEARNING_RATE` â€” TanulÃ¡si rÃ¡ta (alap: 2e-5).

  "data": { "text": "A bekezdÃ©s szÃ¶vege..." },- `MAX_LENGTH` â€” MaximÃ¡lis szekvencia hossz tokenizÃ¡lÃ¡skor (alap: 512).

  "annotations": [{

    "result": [{**Feature engineering:**

      "value": { "choices": ["KÃ¶nnyen Ã©rthetÅ‘"] }- `ENABLE_EMBEDDINGS` â€” Ha `true`, Sentenceâ€‘BERT beÃ¡gyazÃ¡sok szÃ¡mÃ­tÃ¡sa a 02-es lÃ©pÃ©sben (alap: false).

    }]- `EMBEDDING_MODEL` â€” Embedding modell neve (alap: `paraphrase-multilingual-MiniLM-L12-v2`).

  }]

}## FuttatÃ¡s Dockerrel

```

1) Image build:

---

```powershell

## ğŸ“ Projekt StruktÃºradocker build -t deeplearning_project-legal_text_decoder:1.0 .

```

```

â”œâ”€â”€ Dockerfile, docker-compose.yml       # Containerization2) KontÃ©ner futtatÃ¡sa (PowerShell, GPU-val Ã©s volumekkel):

â”œâ”€â”€ docker-run.sh, docker-run.ps1        # Universal launchers

â”œâ”€â”€ requirements.txt                     # Python deps```powershell

â”‚docker run --rm --gpus all `

â”œâ”€â”€ data/                                # INPUT (volume mount)	-v "C:\Users\nagyp\.vscode\DeepLearning Project\attach_folders\data:/app/data" `

â”œâ”€â”€ output/                              # OUTPUT (volume mount)	-v "C:\Users\nagyp\.vscode\DeepLearning Project\attach_folders\output:/app/output" `

â”‚   â”œâ”€â”€ processed/                       # CSV-k	deeplearning_project-legal_text_decoder:1.0 > training_log.txt 2>&1

â”‚   â”œâ”€â”€ models/                          # Trained models```

â”‚   â”œâ”€â”€ reports/                         # Metrics, plots

â”‚   â”œâ”€â”€ evaluation/, robustness/, explainability/Az Ã¶sszes kimenet az `C:\Users\nagyp\.vscode\DeepLearning Project\attach_folders\output` mappÃ¡ban lesz elÃ©rhetÅ‘ (Windows host oldalon).

â”‚

â””â”€â”€ src/## LokÃ¡lis futtatÃ¡s (opcionÃ¡lis)

    â”œâ”€â”€ run.sh                           # Pipeline orchestrator

    â”œâ”€â”€ 01-07_*.py                       # Training scriptsPython kÃ¶rnyezetben (a `requirements.txt` telepÃ­tÃ©se utÃ¡n) egyenkÃ©nt is futtathatÃ³k a scriptek:

    â”œâ”€â”€ api/app.py                       # REST API

    â””â”€â”€ frontend/app.py                  # Streamlit GUI```powershell

```$env:DATA_DIR = "C:\\path\\to\\data"; $env:OUTPUT_DIR = "C:\\path\\to\\output"; python src/01_data_acquisition_and_analysis.py

python src/02_data_cleansing_and_preparation.py

---python src/03_baseline_model.py

python src/05_defining_evaluation_criteria.py

## âš™ï¸ KÃ¶rnyezeti VÃ¡ltozÃ³k (opcionÃ¡lis)```



```bash## Kimenetek

# AdatkezelÃ©s

DATA_DIR=/app/data- `OUTPUT_DIR/processed/` â€” `train.csv`, `val.csv`, `test.csv` (vagy `processed_data.csv` fallback esetÃ©n) szÃ¶vegstatisztikÃ¡kkal kiegÃ©szÃ­tve

OUTPUT_DIR=/app/output- `OUTPUT_DIR/features/` â€” szÃ¶vegstatisztika Ã¡brÃ¡k (hisztogramok), opcionÃ¡lis `embeddings_*.npy` Ã©s `embeddings_meta.json`

- `OUTPUT_DIR/models/` â€” `baseline_model.pkl` (TF-IDF + LogReg), `transformer_model/` (finomhangolt transformer), `label_mapping.json`

# Baseline- `OUTPUT_DIR/reports/` â€” baseline Ã©s transformer metrikÃ¡k (val/test JSON riportok), `transformer_training_history.png`

TFIDF_MAX_FEATURES=20000- `OUTPUT_DIR/evaluation/` â€” kÃ¼lÃ¶n teszt riport Ã©s konfÃºziÃ³s mÃ¡trix a baseline modellhez

LR_C=1.0- `OUTPUT_DIR/robustness/` â€” robusztussÃ¡gi tesztek eredmÃ©nyei (`robustness_results.json`, `robustness_comparison.png`)

- `OUTPUT_DIR/explainability/` â€” feature importance, predikciÃ³ magyarÃ¡zatok, hibaelemzÃ©s JSON-ben Ã©s Ã¡brÃ¡kban

# Transformer

TRANSFORMER_MODEL=SZTAKI-HLT/hubert-base-cc## MegjegyzÃ©sek Ã©s ismert korlÃ¡tok

BATCH_SIZE=8

EPOCHS=3- A stratifikÃ¡lt split legalÃ¡bb kÃ©t osztÃ¡lyt Ã©s elegendÅ‘ mintÃ¡t igÃ©nyel osztÃ¡lyonkÃ©nt. KevÃ©s minta esetÃ©n hibaÃ¼zenetet kaphatsz.

MAX_LENGTH=512- A Sentenceâ€‘BERT beÃ¡gyazÃ¡sok letÃ¶ltÃ©se internetet Ã©s tÃ¶bb memÃ³riÃ¡t igÃ©nyelhet; alapÃ©rtelmezetten ki van kapcsolva.

- A **transformer modell tanÃ­tÃ¡sa (04_incremental_model_development.py) GPU-t igÃ©nyel** a hatÃ©kony futÃ¡shoz. CPU-n is fut, de sokkal lassabb.

# Embeddings- A transformer modell alapÃ©rtelmezetten a magyar **HuBERT** modellt hasznÃ¡lja, de ez kÃ¶rnyezeti vÃ¡ltozÃ³val mÃ³dosÃ­thatÃ³ mÃ¡s modellekre (pl. `bert-base-multilingual-cased`).

ENABLE_EMBEDDINGS=false- Ha csak a baseline modellt szeretnÃ©d futtatni (gyorsabb, kevesebb erÅ‘forrÃ¡s), egyszerÅ±en tÃ¶rÃ¶ld vagy nevezd Ã¡t a `04_incremental_model_development.py` fÃ¡jlt a pipeline elÅ‘tt.

```

## ğŸš€ Gyors IndÃ­tÃ¡s

---

### âš¡ **LegegyszerÅ±bb mÃ³dszer (UniverzÃ¡lis Script)**

## ğŸŒ ML Service (OPCIONÃLIS - Training utÃ¡n)

**Automatikusan felismeri a platformot Ã©s GPU-t!**

### API + GUI egyÃ¼tt (Docker Compose):

```bash

```bash# Linux/macOS/Windows Git Bash

# IndÃ­tÃ¡sbash docker-run.sh

docker-compose up -d

# Windows PowerShell

# ElÃ©rÃ©s.\docker-run.ps1

Frontend: http://localhost:8501

API Docs: http://localhost:8000/docs# CPU-only kÃ©nyszerÃ­tÃ©s (Windows)

```.\docker-run.ps1 -CpuOnly

```

### KÃ¼lÃ¶n indÃ­tÃ¡s:

---

```bash

# API### ğŸ³ Docker Build (platformfÃ¼ggetlen)

python src/api/app.py

```bash

# Frontend# BÃ¡rmilyen platformon

streamlit run src/frontend/app.pydocker build -t deeplearning_project-legal_text_decoder:1.0 .

``````



**FunkciÃ³k:** ValÃ³s idejÅ± predikciÃ³, 2 modell, vizualizÃ¡ciÃ³k, REST API### ğŸ–¥ï¸ Platform-specifikus futtatÃ¡s (manuÃ¡lis)



---#### ğŸªŸ **Windows (PowerShell)**

```powershell

## ğŸ¯ Platform TÃ¡mogatÃ¡s# GPU-val

docker run --rm --gpus all `

| Platform | Docker | GPU | Script |  -v "${PWD}\data:/app/data" `

|----------|--------|-----|--------|  -v "${PWD}\output:/app/output" `

| **Windows 10/11** | Desktop + WSL2 | NVIDIA (WSL2) | `docker-run.ps1` |  deeplearning_project-legal_text_decoder:1.0

| **Linux** | Engine | NVIDIA natÃ­v | `docker-run.sh` |

| **macOS** | Desktop | âŒ CPU only | `docker-run.sh` |# CPU-only (nincs GPU)

docker run --rm `

**KÃ¶vetelmÃ©nyek:** 16GB RAM, 20GB disk, NVIDIA GPU ajÃ¡nlott  -v "${PWD}\data:/app/data" `

  -v "${PWD}\output:/app/output" `

---  deeplearning_project-legal_text_decoder:1.0

```

## ğŸ› HibaelhÃ¡rÃ­tÃ¡s

#### ğŸ§ **Linux**

| ProblÃ©ma | MegoldÃ¡s |```bash

|----------|----------|# GPU-val

| CUDA not available | EllenÅ‘rizd: `nvidia-smi`, Docker GPU support |docker run --rm --gpus all \

| CUDA out of memory | CsÃ¶kkentsd: `BATCH_SIZE=4`, `MAX_LENGTH=256` |  -v "$(pwd)/data:/app/data" \

| Stratified split hiba | Min. 3-5 pÃ©lda/osztÃ¡ly szÃ¼ksÃ©ges |  -v "$(pwd)/output:/app/output" \

| LassÃº CPU futÃ¡s | HasznÃ¡lj GPU-t vagy tÃ¶rÃ¶ld `04_*.py` |  deeplearning_project-legal_text_decoder:1.0



---# CPU-only

docker run --rm \

## â±ï¸ TeljesÃ­tmÃ©ny  -v "$(pwd)/data:/app/data" \

  -v "$(pwd)/output:/app/output" \

| Modell | Accuracy | Training | GPU Memory |  deeplearning_project-legal_text_decoder:1.0

|--------|----------|----------|------------|```

| Baseline | 60-75% | ~3 min | ~500 MB |

| Transformer | 70-85% | ~40 min | ~2-4 GB |#### ğŸ **macOS**

```bash

---# macOS nem tÃ¡mogat CUDA-t, csak CPU mode

docker run --rm \

## ğŸ“ Licenc  -v "$(pwd)/data:/app/data" \

  -v "$(pwd)/output:/app/output" \

MIT License - LÃ¡sd `LICENSE` fÃ¡jl  deeplearning_project-legal_text_decoder:1.0


# Vagy MPS (Apple Silicon) - ha PyTorch tÃ¡mogatja
docker run --rm \
  -v "$(pwd)/data:/app/data" \
  -v "$(pwd)/output:/app/output" \
  -e PYTORCH_ENABLE_MPS_FALLBACK=1 \
  deeplearning_project-legal_text_decoder:1.0
```

**FutÃ¡si idÅ‘:** ~45-60 perc GPU-val | ~6+ Ã³ra CPU-n (transformer miatt)

**Fontos:** 
- A `data/` kÃ¶nyvtÃ¡r tartalmazza a bemeneti JSON adatokat (host gÃ©pen)
- Az `output/` kÃ¶nyvtÃ¡r a futÃ¡s eredmÃ©nyeit tartalmazza (betanÃ­tott modellek, kÃ©pek, riportok)
- Ezek volume-kÃ©nt csatolÃ³dnak a kontÃ©nerbe (`/app/data` Ã©s `/app/output`)
- A Python scriptek a kontÃ©neren belÃ¼l az `/app/output` mappÃ¡ba mentik az eredmÃ©nyeket
- A `data/` Ã©s `output/` kÃ¶nyvtÃ¡rak **NEM** kerÃ¼lnek Git verziÃ³kezelÃ©s alÃ¡ (`.gitignore`)

### ğŸ¯ Platform KÃ¶vetelmÃ©nyek

| Platform | Docker | GPU Support | Script | AjÃ¡nlott RAM |
|----------|--------|-------------|--------|--------------|
| **Windows 10/11** | Docker Desktop + WSL2 | NVIDIA GPU + WSL2 driver | `docker-run.ps1` | 16GB+ |
| **Linux (Ubuntu/Debian)** | Docker Engine | NVIDIA GPU + nvidia-docker2 | `docker-run.sh` | 16GB+ |
| **macOS (Intel)** | Docker Desktop | âŒ CPU only | `docker-run.sh` | 16GB+ |
| **macOS (Apple Silicon)** | Docker Desktop | âš ï¸ MPS (experimental) | `docker-run.sh` | 16GB+ |

**âœ… Platform-fÃ¼ggetlen jellemzÅ‘k:**
- Docker kontÃ©ner Linux alapÃº (bÃ¡rmilyen host-on fut)
- Python kÃ³d 100% cross-platform
- Volume mounting automatikusan kezelt
- UTF-8 tÃ¡mogatÃ¡s beÃ©pÃ­tve
- GPU automatikus detektÃ¡lÃ¡s

## ğŸ“‹ FÅ‘ lÃ©pÃ©sek (pipeline)

1. **01_data_acquisition_and_analysis.py** â€” JSON adatok betÃ¶ltÃ©se (fÃ¡jl vagy mappa), szÃ¶veg tisztÃ­tÃ¡s, label kinyerÃ©s, stratifikÃ¡lt train/val/test split (60/20/20) Ã©s mentÃ©s CSV-be az OUTPUT_DIR/processed mappÃ¡ba.
2. **02_data_cleansing_and_preparation.py** â€” EgyszerÅ± szÃ¶vegstatisztikÃ¡k (word_count, avg_word_len) hozzÃ¡adÃ¡sa Ã©s opcionÃ¡lis Sentence-BERT beÃ¡gyazÃ¡sok mentÃ©se az OUTPUT_DIR/features mappÃ¡ba.
3. **03_baseline_model.py** â€” Baseline szÃ¶vegklasszifikÃ¡ciÃ³s modell: TFâ€‘IDF + LogisticRegression. Modell mentÃ©se (OUTPUT_DIR/models), metrikÃ¡k mentÃ©se (OUTPUT_DIR/reports).
4. **04_incremental_model_development.py** â€” Transformer alapÃº modell (pl. HuBERT) finomhangolÃ¡sa a jogi szÃ¶vegeken. GPU ajÃ¡nlott! Modell Ã©s tokenizer mentÃ©se (OUTPUT_DIR/models/transformer_model).
5. **05_defining_evaluation_criteria.py** â€” KÃ¼lÃ¶n Ã©rtÃ©kelÅ‘ script a baseline modellre a test spliten (OUTPUT_DIR/evaluation).
6. **06_advanced_evaluation_robustness.py** â€” RobusztussÃ¡gi tesztek: zajjal Ã©s csonkolÃ¡ssal mÃ³dosÃ­tott szÃ¶vegeken Ã©rtÃ©keli a baseline modellt (OUTPUT_DIR/robustness).
7. **07_advanced_evaluation_explainability.py** â€” Modell Ã©rtelmezhetÅ‘sÃ©g: top feature-Ã¶k osztÃ¡lyonkÃ©nt, predikciÃ³ magyarÃ¡zatok, hibaelemzÃ©s (OUTPUT_DIR/explainability).

> A `src/run.sh` sorban futtatja az Ã¶sszes script-et a megadott sorrendben. Dockerben ez az alapÃ©rtelmezett belÃ©pÃ©si pont.

## ğŸ“„ AdatformÃ¡tum

ElvÃ¡rt minimÃ¡lis sÃ©ma egy elemre:

```json
{
	"data": { "text": "A bekezdÃ©s szÃ¶vegeâ€¦" },
	"annotations": [
		{
			"result": [
				{ "value": { "choices": ["KÃ¶nnyen Ã©rthetÅ‘"] } }
			]
		}
	]
}
```

**MegjegyzÃ©s:** Az elsÅ‘ elem elsÅ‘ vÃ¡lasztÃ¡sa kerÃ¼l felhasznÃ¡lÃ¡sra: `annotations[0].result[0].value.choices[0]`

## ğŸ“ Projekt StruktÃºra

```
DeepLearning_Project-Legal_Text_Decoder/
â”œâ”€â”€ Dockerfile                                    # Containerization (cross-platform)
â”œâ”€â”€ .dockerignore                                 # Docker build optimization
â”œâ”€â”€ docker-compose.yml                            # ML Service orchestration
â”œâ”€â”€ docker-run.sh                                 # ğŸš€ Universal launcher (Bash)
â”œâ”€â”€ docker-run.ps1                                # ğŸš€ Universal launcher (PowerShell)
â”œâ”€â”€ requirements.txt                              # Python fÃ¼ggÅ‘sÃ©gek
â”œâ”€â”€ README.md                                     # DokumentÃ¡ciÃ³
â”œâ”€â”€ .gitignore                                    # Git kizÃ¡rÃ¡sok
â”‚
â”œâ”€â”€ data/                                         # INPUT (volume mount)
â”‚   â””â”€â”€ *.json                                    # Jogi szÃ¶veg adatok
â”‚
â”œâ”€â”€ src/                                          # PYTHON SCRIPTEK
â”‚   â”œâ”€â”€ run.sh                                    # Pipeline orchestrator
â”‚   â”œâ”€â”€ run_service.sh                            # Service launcher (Bash)
â”‚   â”œâ”€â”€ run_service.ps1                           # Service launcher (PowerShell)
â”‚   â”‚
â”‚   â”œâ”€â”€ 01_data_acquisition_and_analysis.py       # KÃ¶vetelmÃ©ny #2
â”‚   â”œâ”€â”€ 02_data_cleansing_and_preparation.py      # KÃ¶vetelmÃ©ny #3
â”‚   â”œâ”€â”€ 03_baseline_model.py                      # KÃ¶vetelmÃ©ny #5
â”‚   â”œâ”€â”€ 04_incremental_model_development.py       # KÃ¶vetelmÃ©ny #6
â”‚   â”œâ”€â”€ 05_defining_evaluation_criteria.py        # KÃ¶vetelmÃ©ny #4
â”‚   â”œâ”€â”€ 06_advanced_evaluation_robustness.py      # KÃ¶vetelmÃ©ny #7a
â”‚   â”œâ”€â”€ 07_advanced_evaluation_explainability.py  # KÃ¶vetelmÃ©ny #7b
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                                      # REST API Backend
â”‚   â”‚   â””â”€â”€ app.py                                # KÃ¶vetelmÃ©ny #8a
â”‚   â”‚
â”‚   â””â”€â”€ frontend/                                 # Web GUI
â”‚       â””â”€â”€ app.py                                # KÃ¶vetelmÃ©ny #8b
â”‚
â””â”€â”€ output/                                       # OUTPUT (volume mount)
    â”œâ”€â”€ processed/
    â”‚   â”œâ”€â”€ train.csv               # Training set (60%, szÃ¶vegstatisztikÃ¡kkal)
    â”‚   â”œâ”€â”€ val.csv                 # Validation set (20%)
    â”‚   â””â”€â”€ test.csv                # Test set (20%)
    â”‚
    â”œâ”€â”€ features/
    â”‚   â”œâ”€â”€ train_word_count_hist.png
    â”‚   â”œâ”€â”€ train_avg_word_len_hist.png
    â”‚   â”œâ”€â”€ embeddings_train.npy    (ha ENABLE_EMBEDDINGS=true)
    â”‚   â”œâ”€â”€ embeddings_val.npy
    â”‚   â”œâ”€â”€ embeddings_test.npy
    â”‚   â””â”€â”€ embeddings_meta.json
    â”‚
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ baseline_model.pkl      # Sklearn pipeline
    â”‚   â”œâ”€â”€ label_mapping.json      # Label â†’ ID mapping
    â”‚   â””â”€â”€ transformer_model/      # HuBERT modell
    â”‚       â”œâ”€â”€ config.json
    â”‚       â”œâ”€â”€ pytorch_model.bin
    â”‚       â””â”€â”€ tokenizer files
    â”‚
    â”œâ”€â”€ reports/
    â”‚   â”œâ”€â”€ baseline_val_report.json
    â”‚   â”œâ”€â”€ baseline_test_report.json
    â”‚   â”œâ”€â”€ baseline_test_confusion_matrix.png
    â”‚   â”œâ”€â”€ transformer_test_report.json
    â”‚   â””â”€â”€ transformer_training_history.png
    â”‚
    â”œâ”€â”€ evaluation/
    â”‚   â”œâ”€â”€ baseline_test_report.json
    â”‚   â””â”€â”€ baseline_test_confusion_matrix.png
    â”‚
    â”œâ”€â”€ robustness/
    â”‚   â”œâ”€â”€ robustness_results.json
    â”‚   â””â”€â”€ robustness_comparison.png
    â”‚
    â””â”€â”€ explainability/
        â”œâ”€â”€ feature_importance.json
        â”œâ”€â”€ top_features_per_class.png
        â”œâ”€â”€ prediction_explanations.json
        â””â”€â”€ misclassification_analysis.json
```

## âš™ï¸ KÃ¶rnyezeti vÃ¡ltozÃ³k

### AlapvetÅ‘
| VÃ¡ltozÃ³ | AlapÃ©rtelmezett | LeÃ­rÃ¡s |
|---------|----------------|--------|
| `DATA_DIR` | `/app/data` | Input adatok helye |
| `OUTPUT_DIR` | `/app/output` | Kimenetek helye |

### Baseline
| VÃ¡ltozÃ³ | AlapÃ©rtelmezett | LeÃ­rÃ¡s |
|---------|----------------|--------|
| `TFIDF_MAX_FEATURES` | `20000` | Max TF-IDF feature-Ã¶k szÃ¡ma |
| `TFIDF_NGRAM_MAX` | `2` | N-gram felsÅ‘ hatÃ¡r |
| `LR_C` | `1.0` | RegularizÃ¡ciÃ³s paramÃ©ter |

### Transformer
| VÃ¡ltozÃ³ | AlapÃ©rtelmezett | LeÃ­rÃ¡s |
|---------|----------------|--------|
| `TRANSFORMER_MODEL` | `SZTAKI-HLT/hubert-base-cc` | Transformer modell nÃ©v |
| `BATCH_SIZE` | `8` | Batch mÃ©ret (8GB VRAM-hoz) |
| `EPOCHS` | `3` | Epochok szÃ¡ma |
| `LEARNING_RATE` | `2e-5` | TanulÃ¡si rÃ¡ta |
| `MAX_LENGTH` | `512` | Max token hossz |

### Embeddings
| VÃ¡ltozÃ³ | AlapÃ©rtelmezett | LeÃ­rÃ¡s |
|---------|----------------|--------|
| `ENABLE_EMBEDDINGS` | `false` | Sentence-BERT embeddings be/ki |
| `EMBEDDING_MODEL` | `paraphrase-multilingual-MiniLM-L12-v2` | Embedding modell |

## ï¿½ Kimenetek

```
output/
â”œâ”€â”€ processed/          # train/val/test CSV-k
â”œâ”€â”€ features/           # StatisztikÃ¡k, embeddings
â”œâ”€â”€ models/             # baseline_model.pkl, transformer_model/
â”œâ”€â”€ reports/            # MetrikÃ¡k, confusion matrix
â”œâ”€â”€ evaluation/         # Test eredmÃ©nyek
â”œâ”€â”€ robustness/         # RobusztussÃ¡gi tesztek
â””â”€â”€ explainability/     # Feature importance
```

## ğŸŒ ML Service - API + GUI

**FONTOS:** Ez a szolgÃ¡ltatÃ¡s **KÃœLÃ–N** fut a training pipeline-tÃ³l! ElÅ‘szÃ¶r futtasd le a training pipeline-t, majd utÃ¡na indÃ­tsd el a service-t.

### MiÃ©rt kÃ¼lÃ¶n?

A projekt kiÃ©rtÃ©kelÃ©se az eredeti pipeline futtatÃ¡sÃ¡val tÃ¶rtÃ©nik (lÃ¡sd fent). Az ML service egy **opcionÃ¡lis bÃ³nusz funkciÃ³**, amely lehetÅ‘vÃ© teszi a betanÃ­tott modellek hasznÃ¡latÃ¡t egy webes felÃ¼leten.

### API Backend (FastAPI)

**REST API** a betanÃ­tott modellek kiszolgÃ¡lÃ¡sÃ¡ra:

```bash
# LokÃ¡lisan (Python kÃ¶rnyezetben)
python src/api/app.py

# Docker-rel
docker run -d -p 8000:8000 \
  -v "C:\path\to\output:/app/output:ro" \
  deeplearning_project-legal_text_decoder:1.0 \
  python src/api/app.py
```

**Endpoints:**
- `GET /` - Health check
- `POST /predict` - PredikciÃ³ (JSON: `{"text": "...", "model_type": "baseline"}`)
- `GET /models` - ElÃ©rhetÅ‘ modellek listÃ¡ja
- `GET /docs` - Swagger API dokumentÃ¡ciÃ³

### GUI Frontend (Streamlit)

**Webes felÃ¼let** a modellek interaktÃ­v tesztelÃ©sÃ©hez:

```bash
# LokÃ¡lisan
streamlit run src/frontend/app.py

# Docker Compose (ajÃ¡nlott, API + Frontend egyÃ¼tt)
docker-compose up
```

**ElÃ©rhetÅ‘:** http://localhost:8501

### Gyors indÃ­tÃ¡s scriptek

```powershell
# PowerShell
.\src\run_service.ps1

# Vagy Linux/macOS
bash src/run_service.sh
```

### Docker Compose (legegyszerÅ±bb)

```powershell
# IndÃ­tÃ¡s
docker-compose up -d

# LeÃ¡llÃ­tÃ¡s
docker-compose down
```

**ElÃ©rÃ©s:**
- Frontend: http://localhost:8501
- API Docs: http://localhost:8000/docs

### FunkciÃ³k

âœ… **KÃ©t modell** - Baseline Ã©s Transformer kÃ¶zÃ¶tti vÃ¡ltÃ¡s  
âœ… **ValÃ³s idejÅ± predikciÃ³** - Azonnali Ã©rtÃ©kelÃ©s  
âœ… **VizualizÃ¡ciÃ³k** - ValÃ³szÃ­nÅ±sÃ©g eloszlÃ¡s grafikonok  
âœ… **PÃ©lda szÃ¶vegek** - Gyors tesztelÃ©shez  
âœ… **REST API** - KÃ¼lsÅ‘ alkalmazÃ¡sokbÃ³l is hasznÃ¡lhatÃ³  

---

## ğŸ› HibaelhÃ¡rÃ­tÃ¡s

| ProblÃ©ma | MegoldÃ¡s |
|----------|----------|
| `CUDA not available` | EllenÅ‘rizd: `nvidia-smi`, Docker GPU support |
| `CUDA out of memory` | `-e BATCH_SIZE=4` vagy `-e MAX_LENGTH=256` |
| Stratified split hiba | Min. 3-5 pÃ©lda/osztÃ¡ly szÃ¼ksÃ©ges |
| LassÃº futÃ¡s CPU-n | HasznÃ¡lj GPU-t vagy tÃ¶rÃ¶ld `04_incremental_model_development.py` |
| JSON parsing hiba | EllenÅ‘rizd az AdatformÃ¡tum szekciÃ³t |

## â±ï¸ TeljesÃ­tmÃ©ny

| Modell | Accuracy | Training | GPU | Memory |
|--------|----------|----------|-----|--------|
| Baseline | 60-75% | ~3 min | Nem kell | ~500 MB |
| Transformer | 70-85% | ~40 min | 8GB+ VRAM | ~2-4 GB |

**Teljes pipeline:** ~45-60 min (GPU) | ~6+ Ã³ra (CPU)

## ğŸ“ Licenc

LÃ¡sd: `LICENSE`
