# Legal Text Decoder - Gyors Referencia K√°rtya

## üöÄ Leggyakoribb Parancsok

### 1Ô∏è‚É£ Image Build
```powershell
docker build -t deeplearning_project-legal_text_decoder:1.0 .
```

### 2Ô∏è‚É£ Teljes Pipeline Futtat√°s
```powershell
docker run --rm --gpus all `
  -v "C:\Users\nagyp\.vscode\DeepLearning Project\attach_folders\data:/app/data" `
  -v "C:\Users\nagyp\.vscode\DeepLearning Project\attach_folders\output:/app/output" `
  deeplearning_project-legal_text_decoder:1.0 > training_log.txt 2>&1
```

### 3Ô∏è‚É£ Csak Baseline (Gyorsabb, CPU)
√Åtnevez√©s: `04_train_transformer.py` ‚Üí `04_train_transformer.py.bak`
```powershell
docker run --rm `
  -v "C:\Users\nagyp\.vscode\DeepLearning Project\attach_folders\data:/app/data" `
  -v "C:\Users\nagyp\.vscode\DeepLearning Project\attach_folders\output:/app/output" `
  deeplearning_project-legal_text_decoder:1.0 > training_log.txt 2>&1
```

### 4Ô∏è‚É£ Konfigur√°ci√≥ V√°ltoztat√°s
```powershell
docker run --rm --gpus all `
  -e EPOCHS=5 `
  -e BATCH_SIZE=16 `
  -e LEARNING_RATE=3e-5 `
  -v "C:\Users\nagyp\.vscode\DeepLearning Project\attach_folders\data:/app/data" `
  -v "C:\Users\nagyp\.vscode\DeepLearning Project\attach_folders\output:/app/output" `
  deeplearning_project-legal_text_decoder:1.0
```

---

## üîß K√∂rnyezeti V√°ltoz√≥k Cheat Sheet

### Data & Output
| V√°ltoz√≥ | Alap√©rtelmezett | Le√≠r√°s |
|---------|----------------|--------|
| `DATA_DIR` | `/app/data` | Input adatok helye |
| `OUTPUT_DIR` | `/app/output` | Kimenetek helye |

### Baseline Model (TF-IDF + LogReg)
| V√°ltoz√≥ | Alap√©rtelmezett | Le√≠r√°s |
|---------|----------------|--------|
| `TFIDF_MAX_FEATURES` | `20000` | Max TF-IDF feature-√∂k |
| `TFIDF_NGRAM_MAX` | `2` | N-gram fels≈ë hat√°r |
| `LR_C` | `1.0` | Regulariz√°ci√≥s param√©ter |

### Transformer Model
| V√°ltoz√≥ | Alap√©rtelmezett | Le√≠r√°s |
|---------|----------------|--------|
| `TRANSFORMER_MODEL` | `SZTAKI-HLT/hubert-base-cc` | Modell n√©v |
| `BATCH_SIZE` | `8` | Batch m√©ret |
| `EPOCHS` | `3` | Epochok sz√°ma |
| `LEARNING_RATE` | `2e-5` | Tanul√°si r√°ta |
| `MAX_LENGTH` | `512` | Max token hossz |

### Feature Engineering
| V√°ltoz√≥ | Alap√©rtelmezett | Le√≠r√°s |
|---------|----------------|--------|
| `ENABLE_EMBEDDINGS` | `false` | Sentence-BERT be/ki |
| `EMBEDDING_MODEL` | `paraphrase-multilingual-MiniLM-L12-v2` | Embedding modell |

---

## üìÇ Output Strukt√∫ra

```
output/
‚îú‚îÄ‚îÄ processed/          # CSV adatok (train/val/test)
‚îú‚îÄ‚îÄ features/           # Statisztik√°k, hisztogramok, embeddings
‚îú‚îÄ‚îÄ models/             # baseline_model.pkl, transformer_model/
‚îú‚îÄ‚îÄ reports/            # Metrik√°k JSON-ben, confusion matrix-ok
‚îú‚îÄ‚îÄ evaluation/         # Test eredm√©nyek
‚îú‚îÄ‚îÄ robustness/         # Robusztuss√°gi tesztek
‚îî‚îÄ‚îÄ explainability/     # Feature importance, magyar√°zatok
```

---

## üêõ Gyors Hibaelh√°r√≠t√°s

### GPU nem m≈±k√∂dik
```powershell
# Ellen≈ërizd:
nvidia-smi

# Ha nem m≈±k√∂dik, telep√≠tsd:
# NVIDIA Container Toolkit
```

### Out of Memory
```powershell
# Cs√∂kkentsd a batch size-t:
docker run --rm --gpus all -e BATCH_SIZE=4 ...
```

### Lass√∫ fut√°s
```powershell
# Csak baseline (gyorsabb):
# Nevezd √°t vagy t√∂r√∂ld: 04_train_transformer.py
```

### Stratified Split Hiba
```
# Legal√°bb 3-5 p√©lda kell oszt√°lyonk√©nt
# Ellen≈ërizd az adatokat!
```

---

## üìä Pipeline L√©p√©sek

1. **01_data_processing.py** ‚Üí CSV gener√°l√°s (train/val/test)
2. **02_feature_engineering.py** ‚Üí Statisztik√°k + embeddings
3. **03_train_baseline.py** ‚Üí TF-IDF + LogReg
4. **04_train_transformer.py** ‚Üí HuBERT finomhangol√°s (GPU!)
5. **05_evaluation.py** ‚Üí Test √©rt√©kel√©s
6. **06_robustness_tests.py** ‚Üí Zaj/csonkol√°s tesztek
7. **07_explainability.py** ‚Üí Feature importance, magyar√°zatok

---

## üìà Benchmark Id≈ëk

| L√©p√©s | CPU | GPU (RTX 3080) |
|-------|-----|----------------|
| Data Processing | ~30s | ~30s |
| Feature Engineering | ~1min | ~1min |
| Baseline Training | ~3min | ~3min |
| Transformer Training | ~6h+ | ~30-45min |
| Evaluation | ~30s | ~10s |
| Robustness Tests | ~2min | ~1min |
| Explainability | ~1min | ~30s |
| **TOTAL** | ~6h+ | **~40-55min** |

---

## üîë Kulcs F√°jlok

| F√°jl | C√©l |
|------|-----|
| `Dockerfile` | Docker k√∂rnyezet |
| `requirements.txt` | Python csomagok |
| `src/run.sh` | Pipeline orchestration |
| `src/01-07_*.py` | Pipeline l√©p√©sek |
| `README.md` | Haszn√°lati √∫tmutat√≥ |
| `ARCHITECTURE.md` | Technikai dokument√°ci√≥ |
| `SUBMISSION.md` | Bead√°si dokument√°ci√≥ |

---

## üí° Pro Tippek

### 1. Gyors Iter√°ci√≥
Kommenteld ki a hossz√∫ l√©p√©seket a `run.sh`-ban fejleszt√©s alatt:
```bash
# python 04_train_transformer.py  # Kihagy√°s
```

### 2. Memory Optimization
Ha kev√©s a mem√≥ria:
```powershell
-e BATCH_SIZE=4 -e MAX_LENGTH=256
```

### 3. Quick Test
Csak 1 epoch tesztel√©shez:
```powershell
-e EPOCHS=1
```

### 4. Log Monitoring
Val√≥s idej≈± log k√∂vet√©s:
```powershell
docker logs -f <container_id>
```

### 5. Disk Space
A transformer model nagy (~500MB). Figyelj a disk space-re!

---

## üìû Gyakori K√©rd√©sek

**Q: Mennyi id≈ëbe telik a teljes fut√°s?**  
A: GPU-val ~45-60 perc, CPU-val 6+ √≥ra (transformer miatt).

**Q: Kell-e internet?**  
A: Csak az els≈ë fut√°sn√°l (model let√∂lt√©shez). Ut√°na offline is megy.

**Q: Mekkora GPU kell?**  
A: Min. 8GB VRAM (batch_size=8), aj√°nlott 16GB+.

**Q: M≈±k√∂dik CPU-n?**  
A: Igen, de a transformer nagyon lass√∫. Baseline gyors.

**Q: H√°ny adatra van sz√ºks√©g?**  
A: Min. ~100-200 p√©lda, aj√°nlott 1000+.

**Q: T√°mogat m√°s nyelveket?**  
A: Igen! V√°ltoztasd meg a `TRANSFORMER_MODEL` v√°ltoz√≥t.

---

## üìö Hasznos Linkek

- [PyTorch Docs](https://pytorch.org/docs/)
- [Hugging Face Models](https://huggingface.co/models)
- [Docker GPU Setup](https://docs.docker.com/config/containers/resource_constraints/#gpu)
- [scikit-learn](https://scikit-learn.org/stable/)

---

**K√©sz√ºlt:** 2025. November  
**Quick Reference:** v1.0  
**Projekt:** Legal Text Decoder
